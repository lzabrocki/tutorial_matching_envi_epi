{
  "articles": [
    {
      "path": "coarsened_exact_matching.html",
      "title": "Coarsened Exact Matching",
      "description": "Detailled Script.\n",
      "author": [
        {
          "name": "Tarik Benmarhnia",
          "url": "https://profiles.ucsd.edu/tarik.benmarhnia"
        },
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages and Data Loading\r\nCoarsened Exact Matching\r\nMatching Procedure and Covariates Balance Improvement\r\nAnalysis of Matched Data\r\n\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we provide all steps and R codes required to estimate the effect of heat waves on the number of years of life lost (YLL) using coarsened exact matching. The implementation is done with the fantastic package MatchIt: do not hesitate to explore its very well-made documentation. We also rely on the cobalt package for checking covariates balance. Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu.\r\nRequired Packages and Data Loading\r\nTo reproduce exactly the coarsened_exact_matching.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the coarsened_exact_matching.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(broom) # for cleaning regression outputs\r\nlibrary(MatchIt) # for matching procedures\r\nlibrary(cobalt) # for assessing covariates balance\r\nlibrary(lmtest) # for modifying regression standard errors\r\nlibrary(sandwich) # for robust and cluster robust standard errors\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(DT) # for displaying the data as tables\r\n\r\n\r\n\r\nWe load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\", \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nWe finally load the data:\r\n\r\n\r\n# load the data\r\ndata <-\r\n  readRDS(here::here(\"inputs\", \"1.data\", \"environmental_data.rds\"))\r\n\r\n\r\n\r\nAs a reminder, there are 122 days where an heat wave occurred and 1254 days without heat waves.\r\nCoarsened Exact Matching\r\nWe implement below a coarsened exact matching procedure where:\r\neach day with an heat wave is matched to the most similar day without heat wave according to coarsened covariates. This is a 1:1 nearest neighbor matching without replacement.\r\nWe match units according the three lags of the heat wave indicator, the first lag of ozone coarsened into terciles, nitrogen dioxide first lag coarsened into two bins, the relative humidity coarsened into terciles, the month and the year split into periods (“1990-2000” and “2001-2007”).\r\nWe explored many different types of coarsening and the values we chose seemed to result in the best sample size-covariates balance trade-off.\r\nOnce treated and control units are matched, we assess whether covariates balance has improved.\r\nWe finally estimate the treatment effect.\r\nMatching Procedure and Covariates Balance Improvement\r\nWe implement below the coarsened exact matching procedure:\r\n\r\n\r\n# first we split the year variable into two periods\r\ndata <- data %>%\r\n  mutate(year_binned = as.numeric(as.character(year))) %>%\r\n  mutate(year_binned = case_when(year_binned <= 2000 ~ \"1990-2000\",\r\n                          year_binned > 2000 ~ \"2001-2007\"),\r\n         year = as.factor(year))\r\n\r\n# we set the cut-points for continuous covariates\r\ncutpoints =  list(\r\n  o3_lag_1 = \"q3\",\r\n  no2_lag_1 = 2,\r\n  humidity_relative = \"q3\"\r\n)\r\n\r\n# we implement the matching procedure\r\nmatching_coarsened <-\r\n  matchit(\r\n    heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + \r\n     o3_lag_1 + no2_lag_1 + humidity_relative +\r\n      month + year_binned,\r\n    data = data,\r\n    method = \"cem\",\r\n    cutpoints = cutpoints,\r\n    k2k = TRUE\r\n  )\r\n\r\n# display results\r\nmatching_coarsened\r\n\r\n\r\nA matchit object\r\n - method: Coarsened exact matching\r\n - number of obs.: 1376 (original), 158 (matched)\r\n - target estimand: ATT\r\n - covariates: heat_wave_lag_1, heat_wave_lag_2, heat_wave_lag_3, o3_lag_1, no2_lag_1, humidity_relative, month, year_binned\r\n\r\nThe outputtells us that only 158 units were matched. We then evaluate the covariates balance using the love.plot() function from the cobalt package and the absolute mean difference as the summary statistic. For binary variables, the absolute difference in proportion is computed. For continuous covariates, denoted with a star, the absolute standardized mean difference is computed (the difference is divided by the standard deviation of the variable for treated units before matching).\r\n\r\n\r\nPlease show me the code!\r\n\r\n# first we nicely label covariates\r\ncov_labels <- c(\r\n  heat_wave_lag_1 = \"Heat Wave t-1\",\r\n  heat_wave_lag_2 = \"Heat Wave t-2\",\r\n  heat_wave_lag_3 = \"Heat Wave t-3\",\r\n  o3_lag_1 = \"O3 t-1\",\r\n  o3_lag_2 = \"O3 t-2\",\r\n  o3_lag_3 = \"O3 t-3\",\r\n  no2_lag_1 = \"NO2 t-1\",\r\n  no2_lag_2 = \"NO2 t-2\",\r\n  no2_lag_3 = \"NO2 t-3\",\r\n  humidity_relative = \"Relative Humidity\",\r\n  weekend = \"Weekend\",\r\n  month_august = \"August\",\r\n  month_june = \"June\",\r\n  month_july = \"July\",\r\n  year_1990 = \"1990\",\r\n  year_1991 = \"1991\",\r\n  year_1992 = \"1992\",\r\n  year_1993 = \"1993\",\r\n  year_1994 = \"1994\",\r\n  year_1995 = \"1995\",\r\n  year_1996 = \"1996\",\r\n  year_1997 = \"1997\",\r\n  year_1998 = \"1998\",\r\n  year_1999 = \"1999\",\r\n  year_2000 = \"2000\",\r\n  year_2001 = \"2001\",\r\n  year_2002 = \"2002\",\r\n  year_2003 = \"2003\",\r\n  year_2004 = \"2004\",\r\n  year_2005 = \"2005\",\r\n  year_2006 = \"2006\",\r\n  year_2007 = \"2007\")\r\n\r\n# make the love plot\r\ngraph_love_plot_cm <- love.plot(\r\n   heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3  + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + weekend + month + year,\r\n  data = data,\r\n  estimand = \"ATT\",\r\n  weights = list(\"Matched Data\" = matching_coarsened),\r\n  drop.distance = TRUE,\r\n  abs = TRUE,\r\n  var.order = \"unadjusted\",\r\n  binary = \"raw\",\r\n  s.d.denom = \"treated\",\r\n  thresholds = c(m = .1),\r\n  var.names = cov_labels,\r\n  sample.names = c(\"Initial Data\", \"Matched Data\"),\r\n  shapes = c(\"circle\", \"triangle\"),\r\n  colors = c(my_orange, my_blue),\r\n  stars = \"std\"\r\n) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  xlab(\"Absolute Mean Differences\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_love_plot_cm\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_cm,\r\n  filename = here::here(\r\n    \"inputs\", \"3.outputs\",\r\n    \"2.graphs\",\r\n    \"graph_love_plot_cm.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, we see whether covariates balance has increased for most covariates but for some years. We divided the year variable in only two groups to help increase the sample size. If we increase the number of groups, we do not find similar pairs of treated and control units. We display below the evolution of the average of standardized mean differences for continuous covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_cm[[\"data\"]] %>%\r\n  filter(type == \"Contin.\") %>%\r\n  group_by(Sample) %>%\r\n  summarise(\"Average of Standardized Mean Differences\" = round(mean(stat), 2),\r\n            \"Std. Deviation\" = round(sd(stat), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\nSample\r\nAverage of Standardized Mean Differences\r\nStd. Deviation\r\nInitial Data\r\n0.51\r\n0.30\r\nMatched Data\r\n0.04\r\n0.04\r\n\r\nWe also display below the evolution of the difference in proportions for binary covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_cm[[\"data\"]] %>%\r\n  filter(type == \"Binary\") %>%\r\n  group_by(Sample) %>%\r\n  summarise(\"Average of Proportion Differences\" = round(mean(stat), 2),\r\n            \"Std. Deviation\" = round(sd(stat), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\nSample\r\nAverage of Proportion Differences\r\nStd. Deviation\r\nInitial Data\r\n0.06\r\n0.08\r\nMatched Data\r\n0.02\r\n0.03\r\n\r\nOverall, the balance has clearly improved after matching for continuous covariates but less so for categorical variables.\r\nWe finally save the data on covariates balance in the 3.outputs/1.data/covariates_balance folder.\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_cm[[\"data\"]] %>%\r\n  rename_all(tolower) %>%\r\n  select(-on.border) %>%\r\n  mutate(matching_procedure = \"Coarsened Exact Matching\") %>%\r\n  saveRDS(\r\n    .,\r\n    here::here(\r\n      \"inputs\", \"3.outputs\",\r\n      \"1.data\",\r\n      \"covariates_balance\",\r\n      \"data_cov_balance_cm.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nAnalysis of Matched Data\r\nWe now move to the analysis of the matched datasets. It is very important to note that the target causal estimand is not anymore the average treatment effect on the treated as not all treated units could be matched to similar control units. We retrieve the matched dataset:\r\n\r\n\r\n# we retrieve the matched data\r\ndata_cm <- match.data(matching_coarsened)\r\n\r\n\r\n\r\nTo estimate the treatment effect of heat waves on YLL, we first use a simple regression model where we regress the YLL on the treatment indicator.\r\n\r\n\r\n# we fit the regression model\r\nmodel_cm_wo_cov <- lm(yll ~ heat_wave,\r\n                           data = data_cm,\r\n                           weights = weights)\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_cm_wo_cov <- tidy(coeftest(\r\n  model_cm_wo_cov,\r\n  vcov. = vcovCL,\r\n  cluster = ~ subclass\r\n),\r\nconf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_cm_wo_cov %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n207\r\n60\r\n354\r\n\r\nWe find that the estimate for the treatment is equal to +207 years of life lost. The 95% confidence interval is consistent with effects ranging from +60 up to +354. If we want to increase the precision of our estimate and remove any remaining imbalance in covariates, we can also run a multivariate regression. We adjust below for the same variables used in the propensity score matching procedure and the day of the week:\r\n\r\n\r\n# we fit the regression model\r\nmodel_cm_w_cov <-\r\n  lm(\r\n    yll ~ heat_wave + no2_lag_2 + year,\r\n    data = data_cm,\r\n    weights = weights\r\n  )\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_cm_w_cov <- tidy(coeftest(model_cm_w_cov,\r\n                                  vcov. = vcovCL,\r\n                                  cluster = ~ subclass),\r\n                         conf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_cm_w_cov %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n297\r\n154\r\n440\r\n\r\nWe find that the average effect on the treated is equal to +297 years of life lost. The 95% confidence interval is consistent with effects ranging from +154 up to +440. The width of confidence interval is now equal to 286, which is smaller than the previous interval of 294.\r\nWe finally save the data on coarsened results in the 3.outputs/1.data/analysis_results folder.\r\n\r\n\r\nPlease show me the code!\r\n\r\nbind_rows(\r\n  results_cm_wo_cov,\r\n  results_cm_w_cov) %>%\r\n  mutate(\r\n    procedure = c(\r\n      \"Coarsened Matching without Covariates Adjustment\",\r\n      \"Coarsened Matching with Covariates Adjustment\"),\r\n    sample_size = rep(sum(matching_coarsened[[\"weights\"]]), 2)\r\n  ) %>%\r\n  saveRDS(\r\n    .,\r\n    here::here(\r\n      \"inputs\", \"3.outputs\",\r\n      \"1.data\",\r\n      \"analysis_results\",\r\n      \"data_analysis_cem.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:39:15+02:00"
    },
    {
      "path": "constrained_pair_matching.html",
      "title": "Constrained Pair Matching",
      "description": "Detailled Script.\n",
      "author": [
        {
          "name": "Tarik Benmarhnia",
          "url": "https://profiles.ucsd.edu/tarik.benmarhnia"
        },
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages and Data Loading\r\nMatching Procedure\r\nDefining the Treatment of Interest\r\nDefining Thresholds for Matching Covariates\r\nRunning the Matching Procedure\r\n\r\nChecking Covariates Balance Improvement\r\nAnalysing Results\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we provide all steps and R codes required to estimate the effect of heat waves of the number of years of life lost (YLL) using a recently developed constrained pair matching algorithm. Compared to propensity score matching, we can choose the maximum distance allowed between treated and control units for each covariate. The coding procedure is a bit more involved as it has not been formatted in an R package yet. Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu\r\nRequired Packages and Data Loading\r\nTo reproduce exactly the constrained_pair_matching.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the constrained_pair_matching.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(broom) # for cleaning regression outputs\r\nlibrary(Rcpp) # for running the matching algorithm\r\nlibrary(optmatch) # for matching pairs\r\nlibrary(igraph) # for maximum bipartite matching\r\nlibrary(lmtest) # for modifying regression standard errors\r\nlibrary(sandwich) # for robust and cluster robust standard errors\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(DT) # for displaying the data as tables\r\n\r\n\r\n\r\nWe also have to load the script_time_series_matching_function.R located in the functions folder and which provides the functions used for matching time series:\r\n\r\n\r\n# load matching functions\r\nsource(here::here(\r\n  \"inputs\",\r\n  \"2.functions\",\r\n  \"script_time_series_matching_function.R\"\r\n))\r\n\r\n\r\n\r\nWe load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\",\r\n                  \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nWe finally load the data:\r\n\r\n\r\n# load the data\r\nmatching_data <-\r\n  readRDS(here::here(\"inputs\", \"1.data\", \"environmental_data.rds\")) %>%\r\n  fastDummies::dummy_cols(., select_columns = c(\"month\", \"year\")) %>%\r\n  mutate(month = as.factor(month) %>% as.numeric(.))\r\n\r\n\r\n\r\nMatching Procedure\r\nDefining the Treatment of Interest\r\nWe defined our experiment such that:\r\ntreated units are days where an heat wave occurred in t.\r\ncontrol units are day no heat wave occurred in t.\r\nBelow are the required steps to define treatment variable (is_treated) and select the corresponding treated and control units:\r\n\r\n\r\n# define treatment variable\r\nmatching_data <- matching_data %>%\r\n  mutate(is_treated = ifelse(heat_wave == 1, TRUE, FALSE))\r\n\r\n# subset treated and control units\r\ntreated_units = subset(matching_data, is_treated)\r\ncontrol_units = subset(matching_data,!is_treated)\r\nN_treated = nrow(treated_units)\r\nN_control = nrow(control_units)\r\n\r\n\r\n\r\nThere are 122 treated units and 1254 control units. We display the distribution of of treated and control units through time:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make stripes graph\r\nmatching_data %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"Treated\", \"Control\")) %>%\r\n  ggplot(., aes(x = date, y = 1, fill = is_treated)) +\r\n  geom_tile() +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  facet_wrap(~ year, scales = \"free\") +\r\n  scale_fill_manual(name = \"Daily Observations:\", values = c(my_blue, my_orange)) +\r\n  xlab(\"Date\") + ylab(\"\") +\r\n  theme_tufte() +\r\n  theme(axis.ticks.y = element_blank(),\r\n        axis.text.y = element_blank())\r\n\r\n\r\n\r\n\r\nWe save the matching_data:\r\n\r\n\r\nsaveRDS(\r\n  matching_data,\r\n  here::here(\r\n    \"inputs\", \r\n    \"3.outputs\",\r\n    \"1.data\",\r\n    \"constrained_pair_matching\",\r\n    \"matching_data.rds\"\r\n  )\r\n)\r\n\r\n\r\n\r\nDefining Thresholds for Matching Covariates\r\nFor each relevant covariate, we must define a maximum distance for which a treated unit can be matched to a control unit. We chose the distance to maximize covariates balance but also reach a number of matched treated units large enough to draw our inference upon:\r\nA treated unit can be matched to a control unit for a maximum distance of 4 years.\r\nA treated unit must be matched to a control unit according to weekend/working days.\r\nA treated unit must be matched to a control unit belonging to the same month.\r\nA treated unit must be matched to a control unit for the same three lags of the heat wave dummy.\r\nA treated unit can be matched to a control unit for a maximum distance of 10 percentage points in relative humidity.\r\nA treated unit can be matched to a control unit for a maximum distance of 12 \\(\\mu g/m^3\\) in the three lags of ozone.\r\nA treated unit can be matched to a control unit for a maximum distance of 0 \\(\\mu g/m^3\\) in the three lags of nitrogen dioxide.\r\nBelow is the code to define the relevant thresholds:\r\n\r\n\r\n# we create the scaling list as it is needed for running the algorithm\r\n# but we do not use it\r\n\r\nscaling =  rep(list(1),ncol(matching_data))\r\nnames(scaling) = colnames(matching_data)\r\n\r\n# instead, we manually defined the threshold for each covariate\r\nthresholds = rep(list(Inf),ncol(matching_data))\r\nnames(thresholds) = colnames(matching_data)\r\n\r\n# threshold for year\r\nthresholds$year = 2\r\n\r\n# threshold for weekend\r\nthresholds$weekend = 0\r\n\r\n# thresholds for month\r\nthresholds$month = 0\r\n\r\n# threshold for heat_wave lags\r\nthresholds$heat_wave_lag_1 = 0\r\nthresholds$heat_wave_lag_2 = 0\r\nthresholds$heat_wave_lag_3 = 0\r\n\r\n# threshold for humidity_relative\r\nthresholds$humidity_relative = 10\r\n\r\n# threshold for o3\r\nthresholds$o3_lag_1 = 12\r\nthresholds$o3_lag_2 = 12\r\nthresholds$o3_lag_3 = 12\r\n\r\n# thresholds for no2\r\nthresholds$no2_lag_1 = 9\r\nthresholds$no2_lag_2 = 9\r\nthresholds$no2_lag_3 = 9\r\n\r\n\r\n\r\nRunning the Matching Procedure\r\nWe compute discrepancy matrix and run the matching algorithm:\r\n\r\n\r\n# first we compute the discrepancy matrix\r\ndiscrepancies = discrepancyMatrix(treated_units, control_units, thresholds, scaling)\r\n\r\n# convert matching data to data.frame\r\nmatching_data <- as.data.frame(matching_data)\r\n\r\nrownames(discrepancies) = format(matching_data$date[which(matching_data$is_treated)],\"%Y-%m-%d\")\r\ncolnames(discrepancies) = format(matching_data$date[which(!matching_data$is_treated)],\"%Y-%m-%d\")\r\nrownames(matching_data) = matching_data$date\r\n\r\n# run the fullmatch algorithm\r\nmatched_groups = fullmatch(discrepancies, data = matching_data, remove.unmatchables = TRUE, max.controls = 1)\r\n\r\n# get list of matched  treated-control groups\r\ngroups_labels = unique(matched_groups[!is.na(matched_groups)])\r\ngroups_list = list()\r\nfor (i in 1:length(groups_labels)){\r\n  IDs = names(matched_groups)[(matched_groups==groups_labels[i])]\r\n  groups_list[[i]] = as.Date(IDs[!is.na(IDs)])\r\n}\r\n\r\n\r\n\r\nFor some cases, several controls units were matched to a treatment unit. We use the igraph package to force pair matching via bipartite maximal weighted matching. Below is the required code:\r\n\r\n\r\n# we build a bipartite graph with one layer of treated nodes, and another layer of control nodes.\r\n# the nodes are labeled by integers from 1 to (N_treated + N_control)\r\n# by convention, the first N_treated nodes correspond to the treated units, and the remaining N_control\r\n# nodes correspond to the control units.\r\n\r\n# build pseudo-adjacency matrix: edge if and only if match is admissible\r\n# NB: this matrix is rectangular so it is not per say the adjacendy matrix of the graph\r\n# (for this bipartite graph, the adjacency matrix had four blocks: the upper-left block of size\r\n# N_treated by N_treated filled with 0's, bottom-right block of size N_control by N_control filled with 0's,\r\n# top-right block of size N_treated by N_control corresponding to adj defined below, and bottom-left block\r\n# of size N_control by N_treated corresponding to the transpose of adj)\r\nadj = (discrepancies<Inf)\r\n\r\n# extract endpoints of edges\r\nedges_mat = which(adj,arr.ind = TRUE)\r\n\r\n# build weights, listed in the same order as the edges (we use a decreasing function x --> 1/(1+x) to\r\n# have weights inversely proportional to the discrepancies, since maximum.bipartite.matching\r\n# maximizes the total weight and we want to minimize the discrepancy)\r\nweights = 1/(1+sapply(1:nrow(edges_mat),function(i)discrepancies[edges_mat[i,1],edges_mat[i,2]]))\r\n\r\n# format list of edges (encoded as a vector resulting from concatenating the end points of each edge)\r\n# i.e c(edge1_endpoint1, edge1_endpoint2, edge2_endpoint1, edge2_endpoint1, edge3_endpoint1, etc...)\r\nedges_mat[,\"col\"] = edges_mat[,\"col\"] + N_treated\r\nedges_vector = c(t(edges_mat))\r\n\r\n# NB: by convention, the first N_treated nodes correspond to the treated units, and the remaining N_control\r\n# nodes correspond to the control units (hence the \"+ N_treated\" to shift the labels of the control nodes)\r\n\r\n# build the graph from the list of edges\r\nBG = make_bipartite_graph(c(rep(TRUE,N_treated),rep(FALSE,N_control)), edges = edges_vector)\r\n\r\n# find the maximal weighted matching\r\nMBM = maximum.bipartite.matching(BG, weights = weights)\r\n\r\n# list the dates of the matched pairs\r\npairs_list = list()\r\nN_matched = 0\r\nfor (i in 1:N_treated){\r\n  if (!is.na(MBM$matching[i])){\r\n    N_matched = N_matched + 1\r\n    pairs_list[[N_matched]] = c(treated_units$date[i],control_units$date[MBM$matching[i]-N_treated])\r\n  }\r\n}\r\n\r\n# transform the list of matched pairs to a dataframe\r\nmatched_pairs <- enframe(pairs_list) %>%\r\n  unnest(cols = \"value\") %>%\r\n  rename(pair_number = name,\r\n         date = value)\r\n\r\n\r\n\r\nThe hypothetical experiment we set up had 122 treated units and 1254 control units. The matching procedure results in 44 matched treated units.\r\nWe finally merge the matched_pairs with the matching_matching_data to retrieve covariates values for the matched pairs and save the data:\r\n\r\n\r\n# select the matched data for the analysis\r\nfinal_data <-\r\n  left_join(matched_pairs, matching_data, by = \"date\") %>%\r\n  mutate(pair_number = as.factor(pair_number))\r\n\r\n# save the matched data\r\nsaveRDS(\r\n  final_data,\r\n  here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"1.data\",\r\n    \"constrained_pair_matching\",\r\n    \"matched_data.Rds\"\r\n  )\r\n)\r\n\r\n\r\n\r\nChecking Covariates Balance Improvement\r\nWe first bind the matching and matched data together:\r\n\r\n\r\n# bind the two datasets\r\nmatching_data <- matching_data %>%\r\n  mutate(dataset = \"Initial Data\")\r\n\r\nfinal_data <- final_data %>%\r\n    mutate(dataset = \"Matched Data\")\r\n\r\ndata <- bind_rows(matching_data, final_data)\r\n\r\n\r\n\r\nWe change labels of the is_treated variable :\r\n\r\n\r\ndata <- data %>%\r\n  mutate(is_treated = ifelse(is_treated == \"TRUE\", \"True\", \"False\"))\r\n\r\n\r\n\r\nWe then assess covariates balance using a love plot:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute absolute standardized mean differences for continuous covariates\r\ndata_cov_continuous <- data %>%\r\n  select(dataset, is_treated, humidity_relative, o3_lag_1:o3_lag_3, no2_lag_1:no2_lag_3) %>%\r\n  pivot_longer(\r\n    cols = -c(is_treated, dataset),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  select(dataset, is_treated, variable, values)\r\n\r\ndata_abs_difference_continuous <- data_cov_continuous %>%\r\n  group_by(dataset, variable, is_treated) %>%\r\n  summarise(mean_values = mean(values, na.rm = TRUE)) %>%\r\n  summarise(abs_difference = abs(mean_values[2] - mean_values[1]))\r\n\r\ndata_sd_continuous <- data_cov_continuous %>%\r\n  filter(dataset == \"Initial Data\" & is_treated == \"True\") %>%\r\n  group_by(variable) %>%\r\n  summarise(sd_treatment = sd(values, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  select(variable, sd_treatment)\r\n\r\ndata_love_continuous <-\r\n  left_join(data_abs_difference_continuous, data_sd_continuous, by = c(\"variable\")) %>%\r\n  mutate(standardized_difference = abs_difference / sd_treatment) %>%\r\n  select(-c(abs_difference, sd_treatment)) %>%\r\n  mutate(type = \"continuous\")\r\n\r\n\r\n# compute absolute raw mean differences for binary covariates\r\ndata_cov_binary <- data %>%\r\n  select(dataset, is_treated, heat_wave_lag_1:heat_wave_lag_3, weekend, month_june:year_2007) %>%\r\n  pivot_longer(\r\n    cols = -c(is_treated, dataset),\r\n    names_to = \"variable\",\r\n    values_to = \"values\"\r\n  ) %>%\r\n  select(dataset, is_treated, variable, values)\r\n\r\ndata_love_binary <- data_cov_binary %>%\r\n  group_by(dataset, variable, is_treated) %>%\r\n  summarise(mean_values = mean(values, na.rm = TRUE)) %>%\r\n  summarise(standardized_difference = abs(mean_values[2] - mean_values[1])) %>%\r\n  mutate(type = \"binary\")\r\n\r\n\r\n# combine the two datasets\r\ndata_love <- bind_rows(data_love_continuous, data_love_binary)\r\n\r\n# add variable labels\r\ndata_love <- data_love %>%\r\n  mutate(\r\n    variable = case_when(\r\n      variable == \"heat_wave_lag_1\" ~ \"Heat Wave t-1\",\r\n      variable == \"heat_wave_lag_2\" ~ \"Heat Wave t-2\",\r\n      variable == \"heat_wave_lag_3\" ~ \"Heat Wave t-3\",\r\n      variable == \"humidity_relative\" ~ \"Relative Humidity*\",\r\n      variable == \"weekend\" ~ \"Weekend\",\r\n      variable == \"month_august\" ~ \"August\",\r\n      variable == \"month_july\" ~ \"July\",\r\n      variable == \"month_june\" ~ \"June\",\r\n      variable == \"no2_lag_1\" ~ \"NO2 t-1*\",\r\n      variable == \"no2_lag_2\" ~ \"NO2 t-2*\",\r\n      variable == \"no2_lag_3\" ~ \"NO2 t-3*\",\r\n      variable == \"o3_lag_1\" ~ \"O3 t-1*\",\r\n      variable == \"o3_lag_2\" ~ \"O3 t-2*\",\r\n      variable == \"o3_lag_3\" ~ \"O3 t-3*\",\r\n      variable == \"year_1990\" ~ \"1990\",\r\n      variable == \"year_1991\" ~ \"1991\",\r\n      variable == \"year_1992\" ~ \"1992\",\r\n      variable == \"year_1993\" ~ \"1993\",\r\n      variable == \"year_1994\" ~ \"1994\",\r\n      variable == \"year_1995\" ~ \"1995\",\r\n      variable == \"year_1996\" ~ \"1996\",\r\n      variable == \"year_1997\" ~ \"1997\",\r\n      variable == \"year_1998\" ~ \"1998\",\r\n      variable == \"year_1999\" ~ \"1999\",\r\n      variable == \"year_2000\" ~ \"2000\",\r\n      variable == \"year_2001\" ~ \"2001\",\r\n      variable == \"year_2002\" ~ \"2002\",\r\n      variable == \"year_2003\" ~ \"2003\",\r\n      variable == \"year_2004\" ~ \"2004\",\r\n      variable == \"year_2005\" ~ \"2005\",\r\n      variable == \"year_2006\" ~ \"2006\",\r\n      variable == \"year_2007\" ~ \"2007\"\r\n    )\r\n  )\r\n\r\n# arrange the dataset\r\ndata_love <- data_love %>%\r\n  arrange(dataset, standardized_difference) %>% \r\n  mutate(variable=factor(variable, levels=variable))\r\n\r\n# make the graph\r\ngraph_cpm_love_plot_cobalt <- ggplot(data_love, aes(y = variable, x = standardized_difference, colour = fct_rev(dataset), shape = fct_rev(dataset))) +\r\n  geom_vline(xintercept = 0, size = 0.3) +\r\n  geom_vline(xintercept = 0.1, color = \"black\", linetype = \"dashed\") +\r\n  geom_point(size = 4, alpha = 0.8) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 12)) +\r\n  scale_colour_manual(name = \"Dataset:\", values = c(my_blue, my_orange)) +\r\n  scale_shape_manual(name = \"Dataset:\", values = c(17, 16)) +\r\n  xlab(\"Standardized Mean Differences\") +\r\n  ylab(\"\") + \r\n  theme_tufte() + \r\n  theme(plot.margin = margin(t = 0.25, r = 5, b = 0.25, l = 0, unit = \"cm\"))\r\n\r\n# display the graph\r\ngraph_cpm_love_plot_cobalt\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_cpm_love_plot_cobalt,\r\n  filename = here::here(\r\n    \"inputs\",\r\n    \"3.outputs\",\r\n    \"2.graphs\",\r\n    \"graph_cpm_love_plot_cobalt.pdf\"\r\n  ),\r\n  width = 30,\r\n  height = 18,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe display below the evolution of the average of standardized mean differences for continuous covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ndata_love %>%\r\n  filter(type == \"continuous\") %>%\r\n  group_by(dataset) %>%\r\n  summarise(\"Average of Standardized Mean Differences\" = round(mean(standardized_difference), 2),\r\n            \"Std. Deviation\" = round(sd(standardized_difference), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\ndataset\r\nAverage of Standardized Mean Differences\r\nStd. Deviation\r\nInitial Data\r\n0.51\r\n0.30\r\nMatched Data\r\n0.07\r\n0.05\r\n\r\nWe also display below the evolution of the difference in proportions for binary covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ndata_love %>%\r\n  filter(type == \"binary\") %>%\r\n  group_by(dataset) %>%\r\n  summarise(\"Average of Proportion Differences\" = round(mean(standardized_difference), 2),\r\n            \"Std. Deviation\" = round(sd(standardized_difference), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\ndataset\r\nAverage of Proportion Differences\r\nStd. Deviation\r\nInitial Data\r\n0.06\r\n0.08\r\nMatched Data\r\n0.03\r\n0.03\r\n\r\nOverall, the balance has improved for continuous covariates after matching. It is not really the case for binary variables.\r\nWe finally save the data on covariates balance in the 3.outputs/1.data/covariates_balance folder.\r\n\r\n\r\nPlease show me the code!\r\n\r\ndata_love %>%\r\n  rename(sample = dataset, var = variable, stat = standardized_difference) %>%\r\n  mutate(matching_procedure = \"Constrained Pair Matching\") %>%\r\n  saveRDS(\r\n    .,\r\n    here::here(\r\n          \"inputs\",\r\n      \"3.outputs\",\r\n      \"1.data\",\r\n      \"covariates_balance\",\r\n      \"data_cov_balance_cpm.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nAnalysing Results\r\nWe must be careful when we analyze the results as only 44 treated units were matched: the estimand is no longer the ATT. We compute the estimate using a simple linear regression model:\r\n\r\n\r\n# we fit the regression model\r\nmodel_cpm_wo_cov <-\r\n  lm(\r\n    yll ~ is_treated,\r\n    data = final_data\r\n  )\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_cpm_wo_cov <- tidy(coeftest(\r\n  model_cpm_wo_cov,\r\n  vcov. = vcovCL,\r\n  cluster = ~ pair_number\r\n), conf.int = TRUE) %>%\r\n  filter(term == \"is_treatedTRUE\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_cpm_wo_cov %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nis_treatedTRUE\r\n370\r\n190\r\n550\r\n\r\nWe find that the average effect on the treated is equal to +370 years of life lost. The 95% confidence interval is however wide and is consistent with effects ranging from +190 up to +550. If we want to increase the precision of our estimate and remove any remaining imbalance in covariates, we can also run a multivariate regression. We adjust below for the same variables used in the propensity score matching procedure and the day of the week:\r\n\r\n\r\n# we fit the regression model\r\nmodel_cpm_w_cov <-\r\n  lm(\r\n    yll ~ heat_wave + o3_lag_1 + o3_lag_2,\r\n    data = final_data)\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_cpm_w_cov <- tidy(coeftest(\r\n  model_cpm_w_cov,\r\n  vcov. = vcovCL,\r\n  cluster = ~ pair_number\r\n), conf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_cpm_w_cov %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n335\r\n159\r\n511\r\n\r\nWe find that the average effect on the treated is equal to +335 years of life lost. The 95% confidence interval is consistent with effects ranging from +159 up to +511. The width of confidence interval is now equal to 352, which is smaller than the previous interval of 360.\r\nWe finally save the data on constrained pair matching results in the 3.outputs/1.data/analysis_results folder.\r\n\r\n\r\nPlease show me the code!\r\n\r\nbind_rows(results_cpm_wo_cov, results_cpm_w_cov) %>%\r\n  mutate(\r\n    procedure = c(\"Constrained Pair Matching without Covariates Adjustment\", \"Constrained Pair Matching with Covariates Adjustment\"),\r\n    sample_size = rep(N_matched*2, 2)\r\n  ) %>%\r\n  saveRDS(\r\n    .,\r\n    here::here(\r\n          \"inputs\",\r\n      \"3.outputs\",\r\n      \"1.data\",\r\n      \"analysis_results\",\r\n      \"data_analysis_cpm.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:40:12+02:00"
    },
    {
      "path": "eda_covariates_balance.html",
      "title": "Assessing Covariates Balance",
      "description": "Detailled Script.\n",
      "author": [
        {
          "name": "Tarik Benmarhnia",
          "url": "https://profiles.ucsd.edu/tarik.benmarhnia"
        },
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nChecking Covariates Balance\r\nContinuous Covariates\r\nCategorical Covariates\r\n\r\nLack of Common Support\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we provide all steps and R codes required to evaluate if days with heat wave are similar to days without heat wave for a set of confounding factors. Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu\r\nRequired Packages\r\nTo reproduce exactly the eda_covariates_balance.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the eda_covariates_balance.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(broom) # for cleaning regression outputs\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(DT) # for displaying the data as tables\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\", \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nChecking Covariates Balance\r\nWe load the environmental data:\r\n\r\n\r\n# load the data\r\ndata <-\r\n  readRDS(here::here(\"inputs\", \"1.data\", \"environmental_data.rds\")) %>%\r\n  # we recode the heat_wave variable\r\n  mutate(heat_wave = ifelse(heat_wave == 1, \"Days with Heat Wave\", \"Days without Heat Wave\"))\r\n\r\n\r\n\r\nContinuous Covariates\r\nWe first explore whether the relative humidity measured in \\(t\\) and the lags of O\\(_{3}\\) and NO\\(_{2}\\) concentrations are balanced. We plot below the density distribution of each covariate by treatment group:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make the graph\r\ngraph_continuous_cov_densities <- data %>%\r\n  # pivot covariates to long format\r\n  pivot_longer(\r\n    cols = c(humidity_relative, o3_lag_1:o3_lag_3, no2_lag_1:no2_lag_3),\r\n    names_to = \"covariate\",\r\n    values_to = \"value\"\r\n  ) %>%\r\n  # change covariate names\r\n  mutate(\r\n    covariate = case_when(\r\n      covariate == \"humidity_relative\" ~ \"Relative Humidity (%)\",\r\n      covariate == \"o3_lag_1\" ~ \"O3 in t-1 (µg/m³)\",\r\n      covariate == \"o3_lag_2\" ~ \"O3 in t-2 (µg/m³)\",\r\n      covariate == \"o3_lag_3\" ~ \"O3 in t-3 (µg/m³)\",\r\n      covariate == \"no2_lag_1\" ~ \"NO2 in t-1 (µg/m³)\",\r\n      covariate == \"no2_lag_2\" ~ \"NO2 in t-2 (µg/m³)\",\r\n      covariate == \"no2_lag_3\" ~ \"NO2 in t-3 (µg/m³)\"\r\n    )\r\n  ) %>%\r\n  # reorder covariates\r\n  mutate(\r\n    covariate = fct_relevel(\r\n      covariate,\r\n      \"Relative Humidity (%)\",\r\n      \"O3 in t-1 (µg/m³)\",\r\n      \"O3 in t-2 (µg/m³)\",\r\n      \"O3 in t-3 (µg/m³)\",\r\n      \"NO2 in t-1 (µg/m³)\",\r\n      \"NO2 in t-2 (µg/m³)\",\r\n      \"NO2 in t-3 (µg/m³)\"\r\n    )\r\n  ) %>%\r\n  # make density graph\r\n  ggplot(., aes(x = value,\r\n                color = fct_rev(heat_wave))) +\r\n  geom_density() +\r\n  scale_color_manual(name = \"Group:\", values = c(my_blue, my_orange)) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\r\n  facet_wrap(~ covariate, scales = \"free\") +\r\n  xlab(\"Covariate Value\") + ylab(\"Density\") +\r\n  ggtitle(\"Density Distribution of Continuous Covariates by Treatment\") + \r\n  theme_tufte() +\r\n  theme(axis.ticks.y = element_blank(),\r\n        axis.text.y = element_blank())\r\n\r\n# display the graph\r\ngraph_continuous_cov_densities\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_continuous_cov_densities + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_continuous_cov_densities.pdf\"),\r\n  width = 20,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, we can see that the relative humidity and the lags of O\\(_{3}\\) are imbalanced across the treatment and control groups. It is less the case for NO\\(_{2}\\). As an alternative to density distributions, we can summarize the imbalance by computing, for each covariate, the absolute standardized mean difference between treatment and control groups. The absolute standardized mean difference of a covariate is just the absolute value of the difference in means between treated and control units divided by the standard deviation of the treatment group. We can simply compute and plot this metric using the following code:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# reshape the data into long\r\ndata_continuous_cov <- data %>%\r\n  select(heat_wave, humidity_relative, o3_lag_1:o3_lag_3, no2_lag_1:no2_lag_3) %>%\r\n  pivot_longer(cols = -c(heat_wave),\r\n               names_to = \"variable\",\r\n               values_to = \"value\") %>%\r\n  mutate(\r\n    covariate_name = NA %>%\r\n      ifelse(str_detect(variable, \"o3\"), \"O3\", .) %>%\r\n      ifelse(\r\n        str_detect(variable, \"humidity_relative\"),\r\n        \"Relative Humidity\",\r\n        .\r\n      ) %>%\r\n      ifelse(str_detect(variable, \"no2\"), \"NO2\", .)\r\n  ) %>%\r\n  mutate(\r\n    time = \"Lag 0\" %>%\r\n      ifelse(str_detect(variable, \"lag_1\"), \"Lag 1\", .) %>%\r\n      ifelse(str_detect(variable, \"lag_2\"), \"Lag 2\", .) %>%\r\n      ifelse(str_detect(variable, \"lag_3\"), \"Lag 3\", .)\r\n  ) %>%\r\n  mutate(time = fct_relevel(time, \"Lag 3\", \"Lag 2\", \"Lag 1\", \"Lag 0\")) %>%\r\n  select(heat_wave, covariate_name, time, value)\r\n\r\n# compute absolute difference in  means\r\ndata_abs_difference <- data_continuous_cov %>%\r\n  group_by(covariate_name, time, heat_wave) %>%\r\n  summarise(mean_value = mean(value, na.rm = TRUE)) %>%\r\n  summarise(abs_difference = abs(mean_value[2] - mean_value[1]))\r\n\r\n# compute treatment covariates standard deviation\r\ndata_sd <-  data_continuous_cov %>%\r\n  filter(heat_wave == \"Days with Heat Wave\") %>%\r\n  group_by(covariate_name, time, heat_wave) %>%\r\n  summarise(sd_treatment = sd(value, na.rm = TRUE)) %>%\r\n  ungroup() %>%\r\n  select(covariate_name, time, sd_treatment)\r\n\r\n# compute standardized differences\r\ndata_standardized_difference <-\r\n  left_join(data_abs_difference, data_sd, by = c(\"covariate_name\", \"time\")) %>%\r\n  mutate(standardized_difference = abs_difference / sd_treatment) %>%\r\n  select(-c(abs_difference, sd_treatment))\r\n\r\n# make the graph\r\ngraph_std_diff_continuous_cov <- ggplot(data_standardized_difference, aes(y = covariate_name, x = standardized_difference)) +\r\n  geom_vline(xintercept = 0, size = 0.3) +\r\n  geom_vline(xintercept = 0.1, color = my_orange) +\r\n  geom_point(size = 2, color = my_blue) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\r\n  facet_wrap(~ fct_rev(time), nrow = 1) +\r\n  xlab(\"Standardized Mean Differences\") +\r\n  ylab(\"\") + \r\n  ggtitle(\"Standardized Mean Differences by Covariate\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_std_diff_continuous_cov\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_std_diff_continuous_cov + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_std_diff_continuous_cov.pdf\"),\r\n  width = 20,\r\n  height = 6,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, the black line represents standardized mean differences equal to 0 and the orange line is the 0.1 threshold often used in the matching literature to assess balance. Standardized mean differences below this threshold would indicate good balance. Here, for all covariates and lags, the treatment and control groups are imbalanced.\r\nWe display in the table below the values of the standardized mean differences by covariates and lags:\r\n\r\n\r\nPlease show me the code!\r\n\r\ndata_standardized_difference %>%\r\n  mutate(standardized_difference = round(standardized_difference, 2)) %>%\r\n  arrange(fct_rev(time)) %>%\r\n  rename(\"Covariate\" = \"covariate_name\", \"Time\" = \"time\", \"Standardized Mean Difference\" = \"standardized_difference\") %>%\r\n  kable(., align = c(\"l\", \"l\", \"c\"))\r\n\r\n\r\nCovariate\r\nTime\r\nStandardized Mean Difference\r\nRelative Humidity\r\nLag 0\r\n0.67\r\nNO2\r\nLag 1\r\n0.35\r\nO3\r\nLag 1\r\n1.01\r\nNO2\r\nLag 2\r\n0.26\r\nO3\r\nLag 2\r\n0.72\r\nNO2\r\nLag 3\r\n0.14\r\nO3\r\nLag 3\r\n0.41\r\n\r\nCategorical Covariates\r\nFor calendar variables such as the day of the week, the month and the year, we evaluate balance by plotting the proportions of days with and without heat wave. If heat waves were randomly distributed, there should not be difference in the distribution of the proportions for the two groups. We first plot the distribution of proportions for the day of the week:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute the proportions of observations belonging to each wday by treatment status\r\ndata_weekday <- data %>%\r\n  select(weekday, heat_wave) %>%\r\n  mutate(weekday = str_to_title(weekday)) %>%\r\n  pivot_longer(.,-heat_wave) %>%\r\n  group_by(name, heat_wave, value) %>%\r\n  summarise(n = n()) %>%\r\n  mutate(proportion = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    value = fct_relevel(\r\n      value,\r\n      \"Monday\",\r\n      \"Tuesday\",\r\n      \"Wednesday\",\r\n      \"Thursday\",\r\n      \"Friday\",\r\n      \"Saturday\",\r\n      \"Sunday\"\r\n    )\r\n  )\r\n\r\n# make a dots graph\r\ngraph_weekday_balance <- ggplot(data_weekday,\r\n                                aes(\r\n                                  x = as.factor(value),\r\n                                  y = proportion,\r\n                                  colour = heat_wave,\r\n                                  group = heat_wave\r\n                                )) +\r\n  geom_line(size = 0.5, alpha = 0.3) +\r\n  geom_point(size = 2) +\r\n  scale_colour_manual(values = c(my_blue, my_orange),\r\n                      guide = guide_legend(reverse = FALSE)) +\r\n  ggtitle(\"Proportion of Days with and without Heat Waves by Day of the Week\") +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"Day of the Week\") +\r\n  labs(colour = \"Group:\") +\r\n  theme_tufte() +\r\n  theme(\r\n    legend.position = \"top\",\r\n    legend.justification = \"left\",\r\n    legend.direction = \"horizontal\"\r\n  )\r\n\r\n# display the graph\r\ngraph_weekday_balance\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_weekday_balance + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_weekday_balance.pdf\"),\r\n  width = 16,\r\n  height = 9,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, we can see that there are some differences (in percentage points) in the distribution of units between the two groups across days of the week. The differences are however small—at most 4 percentages points. We then plot the same graph but for the month indicator:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute the proportions of observations belonging to each month by treatment status\r\ndata_month <- data %>%\r\n  select(month, heat_wave) %>%\r\n  mutate(month = str_to_title(month)) %>%\r\n  mutate(month = fct_relevel(month,\r\n                             \"June\",\r\n                             \"July\",\r\n                             \"August\")) %>%\r\n  pivot_longer(., -heat_wave) %>%\r\n  group_by(name, heat_wave, value) %>%\r\n  summarise(n = n()) %>%\r\n  mutate(proportion = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup()\r\n\r\n# make a dots graph\r\ngraph_month_balance <- ggplot(data_month,\r\n                              aes(\r\n                                x = as.factor(value),\r\n                                y = proportion,\r\n                                colour = heat_wave,\r\n                                group = heat_wave\r\n                              )) +\r\n  geom_line(size = 0.5, alpha = 0.3) +\r\n  geom_point(size = 2) +\r\n  scale_colour_manual(values = c(my_blue, my_orange),\r\n                      guide = guide_legend(reverse = FALSE)) +\r\n  ggtitle(\"Proportion of Days with and without Heat Waves by Month\") +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"Month\") +\r\n  labs(colour = \"Group:\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_month_balance\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_month_balance + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_month_balance.pdf\"),\r\n  width = 15,\r\n  height = 8,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe also plot the same graph but for the year variable:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# compute the proportions of observations belonging to each year by treatment status\r\ndata_year <- data %>%\r\n  select(year, heat_wave) %>%\r\n  pivot_longer(.,-heat_wave) %>%\r\n  group_by(name, heat_wave, value) %>%\r\n  summarise(n = n()) %>%\r\n  mutate(proportion = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup()\r\n\r\n# make dots plot\r\ngraph_year_balance <- ggplot(data_year,\r\n         aes(\r\n           x = as.factor(value),\r\n           y = proportion,\r\n           colour = heat_wave,\r\n           group = heat_wave\r\n         )) +\r\n  geom_line(size = 0.5, alpha = 0.3) +\r\n  geom_point(size = 2) +\r\n  scale_colour_manual(values = c(my_blue, my_orange),\r\n                      guide = guide_legend(reverse = FALSE)) +\r\n  ggtitle(\"Proportion of Days with and without Heat Waves by Year\") +\r\n  ylab(\"Proportion (%)\") +\r\n  xlab(\"Year\") +\r\n  labs(colour = \"Group:\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_year_balance\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_year_balance + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_year_balance.pdf\"),\r\n  width = 15,\r\n  height = 8,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nNot surprisingly, we can see on this graph that there were more heat waves on specific years.\r\nTo summarize the imbalance for calendar variables, we can finally compute the difference of proportion (in percentage points) between days with and without heat waves. We compute these differences with the following code:\r\n\r\n\r\n# compute differences in proportion\r\ndata_calendar_difference <- data %>%\r\n  select(heat_wave, weekday, month, year) %>%\r\n  mutate_all( ~ as.character(.)) %>%\r\n  pivot_longer(cols = -c(heat_wave),\r\n               names_to = \"variable\",\r\n               values_to = \"value\") %>%\r\n  mutate(value = str_to_title(value)) %>%\r\n  # group by is_treated, variable and values\r\n  group_by(heat_wave, variable, value) %>%\r\n  # compute the number of observations\r\n  summarise(n = n()) %>%\r\n  # compute the proportion\r\n  mutate(freq = round(n / sum(n) * 100, 0)) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    calendar_variable = NA %>%\r\n      ifelse(str_detect(variable, \"weekday\"), \"Day of the Week\", .) %>%\r\n      ifelse(str_detect(variable, \"month\"), \"Month\", .) %>%\r\n      ifelse(str_detect(variable, \"year\"), \"Year\", .)\r\n  ) %>%\r\n  select(heat_wave, calendar_variable, value, freq) %>%\r\n  pivot_wider(names_from = heat_wave, values_from = freq) %>%\r\n  mutate(abs_difference = abs(`Days with Heat Wave` - `Days without Heat Wave`)) %>%\r\n  # reoder the values of variable for the graph\r\n  mutate(\r\n    value = fct_relevel(\r\n      value,\r\n      \"Monday\",\r\n      \"Tuesday\",\r\n      \"Wednesday\",\r\n      \"Thursday\",\r\n      \"Friday\",\r\n      \"Saturday\",\r\n      \"Sunday\",\r\n      \"June\",\r\n      \"July\",\r\n      \"August\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe plot below the differences in proportion for each calendar indicator:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# plot the differences in proportion for each calendar indicator\r\ngraph_all_calendar_balance <-\r\n  ggplot(data_calendar_difference, aes(x = value, y = abs_difference)) +\r\n  geom_segment(aes(\r\n    x = value,\r\n    xend = value,\r\n    y = 0,\r\n    yend = abs_difference\r\n  ), size = 0.3) +\r\n  geom_point(colour = my_blue, size = 3) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +\r\n  facet_wrap( ~ calendar_variable, scales = \"free_x\", ncol = 1) +\r\n  ggtitle(\r\n    \"Absolute Difference in Calendar Indicators Distribution\\nBetween Days with and without Heat Waves\"\r\n  ) +\r\n  xlab(\"Calendar Indicator\") + ylab(\"Absolute Difference\\n(Percentage Points)\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_all_calendar_balance\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_all_calendar_balance + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_all_calendar_balance.pdf\"),\r\n  width = 18,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe display in the table below the values of the standardized mean differences by covariates and lags:\r\n\r\n\r\nPlease show me the code!\r\n\r\ndata_calendar_difference %>%\r\n  select(calendar_variable, value, abs_difference) %>%\r\n  rename(\"Calendard Variable\" = \"calendar_variable\", \"Value\" = \"value\", \"Absolue Difference in % Points\" = \"abs_difference\") %>%\r\n  kable(., align = c(\"l\", \"l\", \"c\"))\r\n\r\n\r\nCalendard Variable\r\nValue\r\nAbsolue Difference in % Points\r\nMonth\r\nAugust\r\n5\r\nMonth\r\nJuly\r\n6\r\nMonth\r\nJune\r\n1\r\nDay of the Week\r\nFriday\r\n3\r\nDay of the Week\r\nMonday\r\n3\r\nDay of the Week\r\nSaturday\r\n0\r\nDay of the Week\r\nSunday\r\n0\r\nDay of the Week\r\nThursday\r\n2\r\nDay of the Week\r\nTuesday\r\n4\r\nDay of the Week\r\nWednesday\r\n4\r\nYear\r\n1990\r\n2\r\nYear\r\n1991\r\n2\r\nYear\r\n1993\r\n1\r\nYear\r\n1994\r\n3\r\nYear\r\n1995\r\n5\r\nYear\r\n1996\r\n5\r\nYear\r\n1997\r\n4\r\nYear\r\n1998\r\n1\r\nYear\r\n1999\r\n6\r\nYear\r\n2000\r\n5\r\nYear\r\n2001\r\n10\r\nYear\r\n2002\r\n5\r\nYear\r\n2003\r\n0\r\nYear\r\n2004\r\n6\r\nYear\r\n2005\r\n11\r\nYear\r\n2006\r\n2\r\nYear\r\n2007\r\n1\r\nYear\r\n1992\r\nNA\r\n\r\nLack of Common Support\r\nTo illustrate the issue of a lack of common support, we plot the YLL against the concentration of ozone in \\(t-1\\) and color the points according to the heat wave status:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# illustrate lack of support\r\ngraph_lack_support_1 <- ggplot(data, aes(x = o3_lag_1, y = yll, colour = heat_wave)) +\r\n  geom_point(shape = 16, alpha = 0.5) +\r\n  scale_colour_manual(values = c(my_orange, my_blue)) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\r\n  labs(colour = \"Group:\") +\r\n  ggtitle(\"Lack of Support?\") +\r\n  xlab(\"O3 in t-1 (µg/m³)\") + ylab(\"YLL\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_lack_support_1\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_lack_support_1 + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_lack_support_1.pdf\"),\r\n  width = 20,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe can see on this graph that some days without heat waves do not have any similar days with heatwaves in terms of ozone concentrations in \\(t-1\\). We can also reproduce this figure by year:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# illustrate lack of support by year\r\ngraph_lack_support_2 <- ggplot(data, aes(x = o3_lag_1, y = yll, colour = heat_wave)) +\r\n  geom_point(shape = 16, alpha = 0.5) +\r\n  scale_colour_manual(values = c(my_orange, my_blue)) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\r\n  facet_wrap(~ year) + \r\n  labs(colour = \"Group:\") +\r\n  ggtitle(\"Lack of Support?\") +\r\n  xlab(\"O3 in t-1 (µg/m³)\") + ylab(\"YLL\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_lack_support_2\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_lack_support_2 + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_lack_support_2.pdf\"),\r\n  width = 30,\r\n  height = 20,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nAgain, there is a clear lack of common support within each year.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:40:59+02:00"
    },
    {
      "path": "index.html",
      "title": "Matching Methods for Environmental Epidemiology: A Tutorial",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Matching Environmental Epidemiology\r\n          \r\n          \r\n          Home\r\n          Paper\r\n          \r\n          \r\n          Data\r\n           \r\n          ▾\r\n          \r\n          \r\n          Sources and Codebook\r\n          EDA\r\n          \r\n          \r\n          Standard Approach\r\n          \r\n          \r\n          Matching\r\n           \r\n          ▾\r\n          \r\n          \r\n          Propensity Score Matching\r\n          \r\n          Coarsened Exact Matching\r\n          \r\n          Constrained Pair Matching\r\n          \r\n          \r\n          \r\n          Summary\r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Matching Methods for Environmental Epidemiology: A Tutorial\r\n            \r\n            \r\n              \r\n                \r\n                    \r\n                      \r\n                        \r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                        \r\n                      \r\n                    \r\n                  \r\n                                  \r\n            \r\n          \r\n        \r\n\r\n          \r\n            \r\n            a:link {\r\n              color: #0471a6;\r\n            }\r\n\r\n            a:hover {\r\n              color: #f4a261;\r\n            }\r\n\r\n            a:visited {\r\n              color: #0471a6;\r\n            }\r\n\r\n            h1 {\r\n              color: #0471a6;\r\n            }\r\n            \r\n            Hello and welcome!\r\n            This website gathers all the materials for the paper Matching Methods for Environmental Epidemiology: A Tutorial by Tarik Benmarhnia, Marie-Abèle Bind, and Léo Zabrocki.\r\n            To illustrate how matching procedures work, we study the effect of heat waves on daily number of years of life lost (YLL) in Montreal over the 1990-2007 period.\r\n            The website is structured as follows:\r\n            A PDF version of the current version of the paper is accessible through the Paper.\r\n            The Data tab describes the sources of the data and the definition of the variables. It also contains a detailed exploratory data analysis of the covariates balance.\r\n            The Standard Approach tab presents results found with an outcome regression model.\r\n            The Matching tab contains the analysis of three matching procedures: propensity score matching, coarsened exact matching and constrained pair matching.\r\n            The Summary tabs gathers all results found with the different procedures.\r\n            All the R code for this tutorial is available in its GitHub directory.\r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n      \r\n        \r\n          \r\n            \r\n              \r\n            \r\n              Matching Methods for Environmental Epidemiology: A Tutorial\r\n            \r\n            \r\n              \r\n                \r\n                                    \r\n                    \r\n                      \r\n                    \r\n                  \r\n                                    \r\n                    \r\n                      \r\n                    \r\n                  \r\n                                  \r\n              \r\n            \r\n            \r\n              \r\n              a:link {\r\n                color: #0471a6;\r\n              }\r\n\r\n              a:hover {\r\n                color: #f4a261;\r\n              }\r\n\r\n              a:visited {\r\n                color: #0471a6;\r\n              }\r\n\r\n              h1 {\r\n                color: #0471a6;\r\n              }\r\n              \r\n              Hello and welcome!\r\n              This website gathers all the materials for the paper Matching Methods for Environmental Epidemiology: A Tutorial by Tarik Benmarhnia, Marie-Abèle Bind, and Léo Zabrocki.\r\n              To illustrate how matching procedures work, we study the effect of heat waves on daily number of years of life lost (YLL) in Montreal over the 1990-2007 period.\r\n              The website is structured as follows:\r\n              A PDF version of the current version of the paper is accessible through the Paper.\r\n              The Data tab describes the sources of the data and the definition of the variables. It also contains a detailed exploratory data analysis of the covariates balance.\r\n              The Standard Approach tab presents results found with an outcome regression model.\r\n              The Matching tab contains the analysis of three matching procedures: propensity score matching, coarsened exact matching and constrained pair matching.\r\n              The Summary tabs gathers all results found with the different procedures.\r\n              All the R code for this tutorial is available in its GitHub directory.\r\n              \r\n            \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2022-03-27T14:40:59+02:00"
    },
    {
      "path": "outcome_regression_analysis.html",
      "title": "Outcome Regression Analysis",
      "description": "Detailled Script.\n",
      "author": [
        {
          "name": "Tarik Benmarhnia",
          "url": "https://profiles.ucsd.edu/tarik.benmarhnia"
        },
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages\r\nRegression Analysis\r\nGraphical Difference in YLL\r\nCrude Regression Analysis\r\nRegression Analysis with Adjustment for Confounders\r\n\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we provide all steps and R codes required to analyse the effect of heat waves on the number of years of life lost (YLL ) using an outcome regression approach. This is often the standard procedure followed by researchers in environmental epidemiology: covariates balance is not discussed and confounding variables are adjusted for with a multivariate regression model. Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu\r\nRequired Packages\r\nTo reproduce exactly the outcome_regression_analysis.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the outcome_regression_analysis.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(broom) # for cleaning regression outputs\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(DT) # for displaying the data as tables\r\n\r\n\r\n\r\nWe finally load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\", \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nRegression Analysis\r\nWe load the simulated environmental data:\r\n\r\n\r\n# load the data\r\ndata <-\r\n  readRDS(here::here(\"inputs\", \"1.data\", \"environmental_data.rds\"))\r\n\r\n\r\n\r\nGraphical Difference in YLL\r\nTo explore whether heat waves lead to an increase in YLL , we could first compare the distribution of the outcome between days with heat waves and days without heat waves:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make graph\r\ngraph_boxplot_YLL  <- data %>%\r\n  # recode heat_wave variable\r\n  mutate(heat_wave = ifelse(heat_wave == 1, \"Days with Heat Wave\", \"Days without Heat Wave\")) %>%\r\n  # make a boxplots graph\r\n  ggplot(., aes(\r\n    x = fct_rev(heat_wave),\r\n    y = yll,\r\n    color = fct_rev(heat_wave)\r\n  )) +\r\n  geom_boxplot() +\r\n  scale_color_manual(values = c(my_blue, my_orange)) +\r\n  scale_y_continuous(\r\n    breaks = scales::pretty_breaks(n = 8),\r\n    labels = function(x)\r\n      format(x, big.mark = \" \", scientific = FALSE)\r\n  ) +\r\n  xlab(\"Heat Wave Indicator\") +\r\n  ylab(\"Number of Years of Life Lost\") +\r\n  ggtitle(\"Number of Years of Life Lost According to Heat Wave Status\") +\r\n  theme_tufte() +\r\n  guides(color = FALSE)\r\n\r\n# display the graph\r\ngraph_boxplot_YLL \r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_boxplot_YLL + labs(title = NULL),\r\n  filename = here::here(\"inputs\", \"3.outputs\", \"2.graphs\", \"graph_boxplot_YLL .pdf\"),\r\n  width = 10,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, we can see that the median of the number of years of life lost for days with heat waves is higher by 388 YLL than the median of days without heat waves.\r\nCrude Regression Analysis\r\nWe then regress the observed number of years of life lost (y_obs) on the indicator for the occurrence of a heat wave (heat_wave) to get a crude estimate of the difference:\r\n\\(\\text{YLL }_{t} = \\alpha + \\beta\\text{HW}_{t} + \\epsilon_{t}\\)\r\nwhere \\(t\\) is the time index, \\(\\beta\\) is the coefficient of interest and \\(\\epsilon\\) the error term. We implement this model in R with the following code:\r\n\r\n\r\n# run the model and clean output\r\noutput_crude_regression <- data %>%\r\n  lm(yll ~ heat_wave, data = .) %>%\r\n  tidy(., conf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display output\r\noutput_crude_regression %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n387\r\n296\r\n479\r\n\r\nThe estimate for the effect of heat waves on YLL is equal to + 387 and the data are consistent with effects ranging from 296 up to 479.\r\nRegression Analysis with Adjustment for Confounders\r\nWe finally run a regression where we adjust for potential confounders such as calendar variables (i.e., the month, the year and their interaction), weather parameters (i.e., the relative humidity) and lags of air pollutants (i.e., NO\\(_{2}\\) and O\\(_{3}\\)). We also include variables such as the day of the week as it is good predictor of the outcome. This regression model can be written such that:\r\n\\(\\text{YLL }_{t} = \\alpha + \\beta\\text{HW}_{t} + \\theta\\text{Hum}_{t} + \\textbf{P}_{t-1:t-3}\\phi + \\textbf{C}_{t}\\gamma + \\epsilon_{t}\\)\r\nwhere \\(Hum\\) is the relative humidity, \\(P\\) the vector of air pollutants variables and \\(C_{t}\\) the vector of calendar indicators. We implement this model in R with the following code:\r\n\r\n\r\n# run the model and clean ouput\r\noutput_adjusted_regression <- data %>%\r\n  lm(yll ~ heat_wave + heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 +\r\n       humidity_relative + \r\n       o3_lag_1 + o3_lag_2 + o3_lag_3 + \r\n       no2_lag_1 + no2_lag_2 + no2_lag_3 + \r\n       weekday + month*as.factor(year),\r\n     data = .) %>%\r\n  tidy(., conf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0)) \r\n\r\n# display output\r\noutput_adjusted_regression %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n243\r\n145\r\n340\r\n\r\nThe estimate for the effect of heat waves on YLL is equal to + 243 and the data are consistent with effects ranging from 145 up to 340. This regression model may be too simple. We could include other lags of the variables and use a cubic spline function to better model variations in YLL.\r\nWe finally save the results from the two outcome regression models in the 3.outputs/1.data/analysis_results folder:\r\n\r\n\r\nbind_rows(output_crude_regression,\r\n  output_adjusted_regression) %>%\r\n  mutate(\r\n    procedure = c(\r\n      \"Outcome Regression Model without Covariates Adjustment\",\r\n      \"Outcome Regression Model with Covariates Adjustment\"\r\n    ),\r\n    sample_size = nrow(data)) %>%\r\n  saveRDS(\r\n    .,\r\n    here::here(\r\n      \"inputs\",\r\n      \"3.outputs\",\r\n      \"1.data\",\r\n      \"analysis_results\",\r\n      \"data_analysis_regression.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:41:09+02:00"
    },
    {
      "path": "propensity_score_matching.html",
      "title": "Propensity Score Matching",
      "description": "Detailled Script.\n",
      "author": [
        {
          "name": "Tarik Benmarhnia",
          "url": "https://profiles.ucsd.edu/tarik.benmarhnia"
        },
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages and Data Loading\r\nPropensity Score Matching\r\nMatching without a Caliper\r\nMatching with a 0.5 Caliper\r\n\r\nAnalysis of Matched Data\r\nMatched Data without a Caliper\r\nMatched Data with a 0.5 Caliper\r\n\r\nSaving Results\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we provide all steps and R codes required to estimate the effect of heat waves of the number of years of life lost (YLL) using propensity score matching. The implementation is done with the fantastic package MatchIt: do not hesitate to explore its very well-made documentation. We also rely on the cobalt package for checking covariate balance. Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu\r\nRequired Packages and Data Loading\r\nTo reproduce exactly the propensity_score_matching.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the propensity_score_matching.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\nlibrary(broom) # for cleaning regression outputs\r\nlibrary(MatchIt) # for matching procedures\r\nlibrary(cobalt) # for assessing covariates balance\r\nlibrary(lmtest) # for modifying regression standard errors\r\nlibrary(sandwich) # for robust and cluster robust standard errors\r\nlibrary(Cairo) # for printing custom police of graphs\r\nlibrary(DT) # for displaying the data as tables\r\n\r\n\r\n\r\nWe load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\", \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nWe finally load the data:\r\n\r\n\r\n# load the data\r\ndata <-\r\n  readRDS(here::here(\"inputs\", \"1.data\", \"environmental_data.rds\")) %>%\r\n  # define year as factors\r\n  mutate(year = as.factor(year))\r\n\r\n\r\n\r\nAs a reminder, there are 122 days where an heat wave occurred and 1254 days without heat waves.\r\nPropensity Score Matching\r\nWe implement below a propensity score matching procedure where:\r\neach day with an heat wave is matched to the most similar day without heat wave. This is a 1:1 nearest neighbor matching without replacement.\r\nthe distance metric used for the matching is the propensity score which is predicted using a logistic model where we regress the heat wave dummy on its three lags, the three lags of ozone and nitrogen dioxide, the relative humidity, the month and the year.\r\nWe vary the matching distance to see how covariates balance change:\r\nWe first match each treated unit to its closest control unit.\r\nWe then set the maximum distance to be inferior to 0.5 propensity score standard deviation.\r\nOnce treated and control units are matched, we assess whether covariates balance has improved.\r\nWe finally estimate the treatment effect.\r\nMatching without a Caliper\r\nWe first match each treated unit to its closest control unit using the matchit() function:\r\n\r\n\r\n# match without caliper\r\nmatching_ps_inf_caliper <-\r\n  matchit(\r\n    heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 +\r\n      humidity_relative + \r\n      o3_lag_1 + o3_lag_2 + o3_lag_3 +\r\n      no2_lag_1 + no2_lag_2 + no2_lag_3 +\r\n      weekend + month + year,\r\n    data = data\r\n  )\r\n\r\n# display summary of the procedure\r\nmatching_ps_inf_caliper\r\n\r\n\r\nA matchit object\r\n - method: 1:1 nearest neighbor matching without replacement\r\n - distance: Propensity score\r\n             - estimated with logistic regression\r\n - number of obs.: 1376 (original), 244 (matched)\r\n - target estimand: ATT\r\n - covariates: heat_wave_lag_1, heat_wave_lag_2, heat_wave_lag_3, humidity_relative, o3_lag_1, o3_lag_2, o3_lag_3, no2_lag_1, no2_lag_2, no2_lag_3, weekend, month, year\r\n\r\nThe output of the matching procedure indicates us the method (1:1 nearest neighbor matching without replacement) and the distance (propensity score) we used. It also tells us how many units were matched: 244. We assess how covariates balance has improved by comparing the distribution of propensity scores before and after matching:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# distribution of propensity scores\r\ngraph_propensity_score_distribution_inf <- bal.plot(\r\n  matching_ps_inf_caliper,\r\n  var.name = \"distance\",\r\n  which = \"both\",\r\n  sample.names = c(\"Initial Data\", \"Matched Data\"),\r\n  type = \"density\") +\r\n  ggtitle(\"Distribution of the Propensity Score Before and After Matching\") +\r\n  xlab(\"Propensity Scores\") +\r\n  scale_fill_manual(\r\n    name = \"Group:\",\r\n    values = c(my_blue, my_orange),\r\n    labels = c(\"Days without Heat Waves\", \"Days with Heat Waves\")\r\n  ) +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_propensity_score_distribution_inf\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_propensity_score_distribution_inf + labs(title = NULL),\r\n  filename = here::here(\r\n    \"inputs\", \"3.outputs\",\r\n    \"2.graphs\",\r\n    \"graph_propensity_score_distribution_inf.pdf\"\r\n  ),\r\n  width = 16,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nWe see on this graph that propensity scores distribution for the two groups better overlap but the two density distributions are still not similar. We can also evaluate the covariates balance using the love.plot() function from the cobalt package and the absolute mean difference as the summary statistic. For binary variables, the absolute difference in proportion is computed. For continuous covariates, denoted with a star, the absolute standardized mean difference is computed (the difference is divided by the standard deviation of the variable for treated units before matching).\r\n\r\n\r\nPlease show me the code!\r\n\r\n# first we nicely label covariates\r\ncov_labels <- c(\r\n  heat_wave_lag_1 = \"Heat Wave t-1\",\r\n  heat_wave_lag_2 = \"Heat Wave t-2\",\r\n  heat_wave_lag_3 = \"Heat Wave t-3\",\r\n  o3_lag_1 = \"O3 t-1\",\r\n  o3_lag_2 = \"O3 t-2\",\r\n  o3_lag_3 = \"O3 t-3\",\r\n  no2_lag_1 = \"NO2 t-1\",\r\n  no2_lag_2 = \"NO2 t-2\",\r\n  no2_lag_3 = \"NO2 t-3\",\r\n  humidity_relative = \"Relative Humidity\",\r\n  weekend = \"Weekend\",\r\n  month_august = \"August\",\r\n  month_june = \"June\",\r\n  month_july = \"July\",\r\n  year_1990 = \"1990\",\r\n  year_1991 = \"1991\",\r\n  year_1992 = \"1992\",\r\n  year_1993 = \"1993\",\r\n  year_1994 = \"1994\",\r\n  year_1995 = \"1995\",\r\n  year_1996 = \"1996\",\r\n  year_1997 = \"1997\",\r\n  year_1998 = \"1998\",\r\n  year_1999 = \"1999\",\r\n  year_2000 = \"2000\",\r\n  year_2001 = \"2001\",\r\n  year_2002 = \"2002\",\r\n  year_2003 = \"2003\",\r\n  year_2004 = \"2004\",\r\n  year_2005 = \"2005\",\r\n  year_2006 = \"2006\",\r\n  year_2007 = \"2007\"\r\n)\r\n\r\n# make the love plot\r\ngraph_love_plot_ps_inf <- love.plot(\r\n  matching_ps_inf_caliper,\r\n  drop.distance = TRUE,\r\n  abs = TRUE,\r\n  var.order = \"unadjusted\",\r\n  binary = \"raw\",\r\n  s.d.denom = \"treated\",\r\n  thresholds = c(m = .1),\r\n  var.names = cov_labels,\r\n  sample.names = c(\"Initial Data\", \"Matched Data\"),\r\n  shapes = c(\"circle\", \"triangle\"),\r\n  colors = c(my_orange, my_blue),\r\n  stars = \"std\"\r\n) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  xlab(\"Absolute Mean Differences\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_love_plot_ps_inf\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_ps_inf + labs(title = NULL),\r\n  filename = here::here(\r\n    \"inputs\", \"3.outputs\",\r\n    \"2.graphs\",\r\n    \"graph_love_plot_ps_inf.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, we can see that, for most covariates, balance has improved after matching—yet, for few covariates, the standardized mean difference has increased. We display below the evolution of the average of standardized mean differences for continuous covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_ps_inf[[\"data\"]] %>%\r\n  filter(type == \"Contin.\") %>%\r\n  group_by(Sample) %>%\r\n  summarise(\"Average of Standardized Mean Differences\" = round(mean(stat), 2),\r\n            \"Std. Deviation\" = round(sd(stat), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\nSample\r\nAverage of Standardized Mean Differences\r\nStd. Deviation\r\nInitial Data\r\n0.51\r\n0.30\r\nMatched Data\r\n0.06\r\n0.04\r\n\r\nWe also display below the evolution of the difference in proportions for binary covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_ps_inf[[\"data\"]] %>%\r\n  filter(type == \"Binary\") %>%\r\n  group_by(Sample) %>%\r\n  summarise(\"Average of Proportion Differences\" = round(mean(stat), 2),\r\n            \"Std. Deviation\" = round(sd(stat), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\nSample\r\nAverage of Proportion Differences\r\nStd. Deviation\r\nInitial Data\r\n0.06\r\n0.08\r\nMatched Data\r\n0.02\r\n0.02\r\n\r\nOverall, for both types of covariates, the balance has clearly improved after matching.\r\nMatching with a 0.5 Caliper\r\nUntil now, we matched each treated unit to its closest control unit according to 1 standard deviation caliper: we could however make sure that a treated unit is not matched to a control unit which is too much different. We do so by setting a caliper of 0.5 standard deviation:\r\n\r\n\r\n# match without caliper\r\nmatching_ps_0.5_caliper <-\r\n  matchit(\r\n    heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 +\r\n      humidity_relative + \r\n      o3_lag_1 + o3_lag_2 + o3_lag_3 +\r\n      no2_lag_1 + no2_lag_2 + no2_lag_3 +\r\n      weekend + month + year,\r\n    caliper = 0.5,\r\n    data = data\r\n  )\r\n\r\n# display summary of the procedure\r\nmatching_ps_0.5_caliper\r\n\r\n\r\nA matchit object\r\n - method: 1:1 nearest neighbor matching without replacement\r\n - distance: Propensity score [caliper]\r\n             - estimated with logistic regression\r\n - caliper: <distance> (0.073)\r\n - number of obs.: 1376 (original), 236 (matched)\r\n - target estimand: ATT\r\n - covariates: heat_wave_lag_1, heat_wave_lag_2, heat_wave_lag_3, humidity_relative, o3_lag_1, o3_lag_2, o3_lag_3, no2_lag_1, no2_lag_2, no2_lag_3, weekend, month, year\r\n\r\nCompared to the matching with an infinite caliper, there are now 236 matched units. We can check whether the propensity score distributions overlap better:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# distribution of propensity scores\r\ngraph_propensity_score_distribution_0.5 <- bal.plot(\r\n  matching_ps_0.5_caliper,\r\n  var.name = \"distance\",\r\n  which = \"both\",\r\n  sample.names = c(\"Initial Data\", \"Matched Data\"),\r\n  type = \"density\"\r\n) +\r\n  ggtitle(\"Distribution of the Propensity Score Before and After Matching\") +\r\n  xlab(\"Propensity Scores\") +\r\n  scale_fill_manual(\r\n    name = \"Group:\",\r\n    values = c(my_blue, my_orange),\r\n    labels = c(\"Days without Heat Waves\", \"Days with Heat Waves\")\r\n  ) +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_propensity_score_distribution_0.5\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_propensity_score_distribution_0.5 + labs(title = NULL),\r\n  filename = here::here(\r\n    \"inputs\", \"3.outputs\",\r\n    \"2.graphs\",\r\n    \"graph_propensity_score_distribution_0.5.pdf\"\r\n  ),\r\n  width = 16,\r\n  height = 10,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nThe overlap seems to be better than the matching without a caliper. We can also evaluate how each covariate balance has improved with a love plot:\r\n\r\n\r\nPlease show me the code!\r\n\r\n# make the love plot\r\ngraph_love_plot_ps_0.5 <- love.plot(\r\n  heat_wave ~ heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3  + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + weekend + month + year,\r\n  data = data,\r\n  estimand = \"ATT\",\r\n  weights = list(\"Without a Caliper\" = matching_ps_inf_caliper,\r\n                 \"With a 0.5 SD Caliper\" = matching_ps_0.5_caliper),\r\n  drop.distance = TRUE,\r\n  abs = TRUE,\r\n  var.order = \"unadjusted\",\r\n  binary = \"raw\",\r\n  s.d.denom = \"treated\",\r\n  thresholds = c(m = .1),\r\n  var.names = cov_labels,\r\n  sample.names = c(\"Initial Data\", \"Without a Caliper\", \"With a 0.5 SD Caliper\"),\r\n  shapes = c(\"circle\", \"triangle\", \"square\"),\r\n  colors = c(my_orange, my_blue, \"#81b29a\"),\r\n  stars = \"std\"\r\n) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\r\n  xlab(\"Absolute Mean Differences\") +\r\n  theme_tufte()\r\n\r\n# display the graph\r\ngraph_love_plot_ps_0.5\r\n\r\n\r\n\r\nPlease show me the code!\r\n\r\n# save the graph\r\nggsave(\r\n  graph_love_plot_ps_0.5,\r\n  filename = here::here(\r\n    \"inputs\", \"3.outputs\",\r\n    \"2.graphs\",\r\n    \"graph_love_plot_ps_0.5.pdf\"\r\n  ),\r\n  width = 20,\r\n  height = 15,\r\n  units = \"cm\",\r\n  device = cairo_pdf\r\n)\r\n\r\n\r\n\r\nOn this graph, it is clear to see that, for several continuous covariates, balance has increased. We display below, for continuous covariates, the average of standardized mean differences for the three datasets:\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_ps_0.5[[\"data\"]] %>%\r\n  filter(type == \"Contin.\") %>%\r\n  group_by(Sample) %>%\r\n  summarise(\"Average of Standardized Mean Differences\" = round(mean(stat), 2),\r\n            \"Std. Deviation\" = round(sd(stat), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\nSample\r\nAverage of Standardized Mean Differences\r\nStd. Deviation\r\nInitial Data\r\n0.51\r\n0.30\r\nWithout a Caliper\r\n0.06\r\n0.04\r\nWith a 0.5 SD Caliper\r\n0.06\r\n0.02\r\n\r\nWe also display below the evolution of the difference in proportions for binary covariates:\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_ps_0.5[[\"data\"]] %>%\r\n  filter(type == \"Binary\") %>%\r\n  group_by(Sample) %>%\r\n  summarise(\"Average of Proportion Differences\" = round(mean(stat), 2),\r\n            \"Std. Deviation\" = round(sd(stat), 2)) %>%\r\n  kable(align = c(\"l\", \"c\"))\r\n\r\n\r\nSample\r\nAverage of Proportion Differences\r\nStd. Deviation\r\nInitial Data\r\n0.06\r\n0.08\r\nWithout a Caliper\r\n0.02\r\n0.02\r\nWith a 0.5 SD Caliper\r\n0.01\r\n0.01\r\n\r\nHere, the stricter matching procedure did not help improve the balance of binary covariates.\r\nWe finally save the data on covariates balance in the 3.outputs/1.data/covariates_balance folder.\r\n\r\n\r\nPlease show me the code!\r\n\r\ngraph_love_plot_ps_0.5[[\"data\"]] %>%\r\n  rename_all(tolower) %>%\r\n  mutate(matching_procedure = \"Propensity Score Matching\") %>%\r\n  select(-on.border) %>%\r\n  saveRDS(\r\n    .,\r\n    here::here(\r\n      \"inputs\", \"3.outputs\",\r\n      \"1.data\",\r\n      \"covariates_balance\",\r\n      \"data_cov_balance_ps.RDS\"\r\n    )\r\n  )\r\n\r\n\r\n\r\nAnalysis of Matched Data\r\nMatched Data without a Caliper\r\nWe now move to the analysis of the matched datasets using a simple regression model where we first regress the YLL on the treatment indicator. We start with the matched data resulting from the propensity score without a caliper. We first retrieve the matched dataset:\r\n\r\n\r\n# we retrieve the matched data\r\ndata_ps_inf_caliper <- match.data(matching_ps_inf_caliper)\r\n\r\n\r\n\r\nWe then estimate the treatment effect of heat waves with a simple linear regression model:\r\n\r\n\r\n# we fit the regression model\r\nmodel_ps_inf_caliper <- lm(yll ~ heat_wave,\r\n                           data = data_ps_inf_caliper,\r\n                           weights = weights)\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_ps_inf_caliper <- tidy(coeftest(\r\n  model_ps_inf_caliper,\r\n  vcov. = vcovCL,\r\n  cluster = ~ subclass\r\n),\r\nconf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_ps_inf_caliper %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n261\r\n114\r\n409\r\n\r\nWe find that the average effect on the treated is equal to +261 years of life lost. The 95% confidence interval is consistent with effects ranging from +114 up to +409. If we want to increase the precision of our estimate and remove any remaining imbalance in covariates, we can also run a multivariate regression. We adjust below for the same variables used in the estimation of propensity scores and the day of the week:\r\n\r\n\r\n# we fit the regression model\r\nmodel_ps_inf_caliper_w_cov <-\r\n  lm(\r\n    yll ~ heat_wave + heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3 + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + weekend + month + year,\r\n    data = data_ps_inf_caliper,\r\n    weights = weights\r\n  )\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_ps_inf_caliper_w_cov <- tidy(coeftest(\r\n  model_ps_inf_caliper_w_cov,\r\n  vcov. = vcovCL,\r\n  cluster = ~ subclass\r\n),\r\nconf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_ps_inf_caliper_w_cov %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n241\r\n101\r\n380\r\n\r\nWe find that the average effect on the treated is equal to +241 years of life lost. The 95% confidence interval is consistent with effects ranging from +101 up to +380. The width of confidence interval is now equal to 279, which is more than half smaller than the previous interval of 295.\r\nMatched Data with a 0.5 Caliper\r\nWe also estimate the treatment effect for the matched dataset resulting from the matching procedure with a 0.5 caliper. It is very important to note that the target causal estimand is not anymore the the average treatment on the treated as not all treated units could be matched to similar control units: only 118 treated units were matched. We first retrieve the matched dataset:\r\n\r\n\r\n# we retrieve the matched data\r\ndata_ps_0.5_caliper <- match.data(matching_ps_0.5_caliper)\r\n\r\n\r\n\r\nWe estimate the treatment effect of heat waves with a simple linear regression model, we get:\r\n\r\n\r\n# we fit the regression model\r\nmodel_ps_0.5_caliper <- lm(yll ~ heat_wave,\r\n                          data = data_ps_0.5_caliper,\r\n                          weights = weights)\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_ps_0.5_caliper <- tidy(coeftest(\r\n  model_ps_0.5_caliper,\r\n  vcov. = vcovCL,\r\n  cluster = ~ subclass\r\n),\r\nconf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_ps_0.5_caliper %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n265\r\n112\r\n418\r\n\r\nThe estimated is equal to +265 years of life lost. The 95% confidence interval is consistent with effects ranging from +112 up to +418. We finally run the regression model where we adjust for covariates:\r\n\r\n\r\n# we fit the regression model\r\nmodel_ps_0.5_caliper_w_cov <-\r\n  lm(\r\n    yll ~ heat_wave + heat_wave_lag_1 + heat_wave_lag_2 + heat_wave_lag_3 + o3_lag_1 + o3_lag_2 + o3_lag_3 + no2_lag_1 + no2_lag_2 + no2_lag_3 + humidity_relative + weekend + month + year,\r\n    data = data_ps_0.5_caliper,\r\n    weights = weights\r\n  )\r\n\r\n# retrieve the estimate and 95% ci\r\nresults_ps_0.5_caliper_w_cov <- tidy(coeftest(\r\n  model_ps_0.5_caliper_w_cov,\r\n  vcov. = vcovCL,\r\n  cluster = ~ subclass\r\n),\r\nconf.int = TRUE) %>%\r\n  filter(term == \"heat_wave\") %>%\r\n  select(term, estimate, conf.low, conf.high) %>%\r\n  mutate_at(vars(estimate:conf.high), ~ round(., 0))\r\n\r\n# display results\r\nresults_ps_0.5_caliper_w_cov %>%\r\n  rename(\r\n    \"Term\" = term,\r\n    \"Estimate\" = estimate,\r\n    \"95% CI Lower Bound\" = conf.low,\r\n    \"95% CI Upper Bound\" = conf.high\r\n  ) %>%\r\n  kable(., align = c(\"l\", \"c\", \"c\", \"c\"))\r\n\r\n\r\nTerm\r\nEstimate\r\n95% CI Lower Bound\r\n95% CI Upper Bound\r\nheat_wave\r\n251\r\n103\r\n399\r\n\r\nWe find that the average effect on the treated is equal to +251 years of life lost. The 95% confidence interval is consistent with effects ranging from +103 up to +399. The width of confidence interval is now equal to 296, which is more than half smaller than the previous interval of 306.\r\nSaving Results\r\nWe finally save the data on results from propensity score analyses in the 3.outputs/1.data/analysis_results folder.\r\n\r\n\r\nPlease show me the code!\r\n\r\nbind_rows(\r\n  results_ps_inf_caliper,\r\n  results_ps_inf_caliper_w_cov,\r\n  results_ps_0.5_caliper,\r\n  results_ps_0.5_caliper_w_cov\r\n) %>%\r\n  mutate(\r\n    procedure = c(\r\n      \"Propensity Score without a Caliper\",\r\n      \"Propensity Score without a Caliper and with Covariates Adjustment\",\r\n      \"Propensity Score with a 0.5 SD Caliper\",\r\n      \"Propensity Score with a 0.5 SD Caliper and with Covariates Adjustment\"\r\n    ),\r\n    sample_size = c(rep(sum(\r\n      matching_ps_inf_caliper[[\"weights\"]]\r\n    ), 2), rep(sum(\r\n      matching_ps_0.5_caliper[[\"weights\"]]\r\n    ), 2))\r\n  ) %>%\r\n  saveRDS(.,\r\n          here::here(\r\n            \"inputs\", \"3.outputs\",\r\n            \"1.data\",\r\n            \"analysis_results\",\r\n            \"data_analysis_ps.RDS\"\r\n          ))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:41:42+02:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\r\nMatching Methods for Environmental Epidemiology: A Tutorial\r\nThis repository contains all the materials necessary to reproduce our tutorial on matching methods for environmental epidemiology.\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:41:49+02:00"
    },
    {
      "path": "summary_results.html",
      "title": "Summary of Results ",
      "description": "Detailled Script.\n",
      "author": [
        {
          "name": "Tarik Benmarhnia",
          "url": "https://profiles.ucsd.edu/tarik.benmarhnia"
        },
        {
          "name": "Marie-Abèle Bind",
          "url": "https://scholar.harvard.edu/marie-abele"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRequired Packages and Data Loading\r\nCovariates Balance Results\r\nAnalysis Results\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\nIn this document, we display the results of the three matching procedures we implemented. Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu.\r\nRequired Packages and Data Loading\r\nTo reproduce exactly the summary_results.html document, we first need to have installed:\r\nthe R programming language\r\nRStudio, an integrated development environment for R, which will allow you to knit the summary_results.Rmd file and interact with the R code chunks\r\nthe R Markdown package\r\nand the Distill package which provides the template for this document.\r\nOnce everything is set up, we load the following packages:\r\n\r\n\r\n# load required packages\r\nlibrary(knitr) # for creating the R Markdown document\r\nlibrary(here) # for files paths organization\r\nlibrary(tidyverse) # for data manipulation and visualization\r\n\r\n\r\n\r\nWe load our custom ggplot2 theme for graphs:\r\n\r\n\r\n# load ggplot custom theme\r\nsource(here::here(\"inputs\", \"2.functions\",\r\n                  \"script_theme_tufte.R\"))\r\n# define nice colors\r\nmy_blue <- \"#0081a7\"\r\nmy_orange <- \"#fb8500\"\r\n\r\n\r\n\r\nCovariates Balance Results\r\nWe load and clean the data on covariates balance for each matching procedure:\r\n\r\n\r\n# load and bind data\r\nfiles <- dir(\r\n  path = here::here(\"inputs\",\r\n                    \"3.outputs\",\r\n                    \"1.data\",\r\n                    \"covariates_balance\"),\r\n  pattern = \"*.RDS\",\r\n  full.names = TRUE\r\n)\r\n\r\ndata_cov_balance <- files %>%\r\n  map( ~ readRDS(.)) %>%\r\n  reduce(rbind)\r\n\r\n# recode type\r\ndata_cov_balance <- data_cov_balance %>%\r\n  mutate(\r\n    type = case_when(\r\n      type == \"Binary\" ~ \"Binary\",\r\n      type == \"Contin.\" ~ \"Continuous\",\r\n      type == \"binary\" ~ \"Binary\",\r\n      type == \"continuous\" ~ \"Continuous\"\r\n    )\r\n  )\r\n\r\n# clean data for summary statistics\r\ndata_cm_cpm <- data_cov_balance %>%\r\n  filter(matching_procedure %in% c(\"Coarsened Exact Matching\", \"Constrained Pair Matching\")) %>%\r\n  filter(sample == \"Matched Data\") %>%\r\n  select(var, type, stat, matching_procedure)\r\n\r\ndata_initial <- data_cov_balance %>%\r\n  filter(matching_procedure == \"Coarsened Exact Matching\" &\r\n           sample == \"Initial Data\") %>%\r\n  select(-matching_procedure) %>%\r\n  rename(matching_procedure = sample)\r\n\r\ndata_ps <- data_cov_balance %>%\r\n  filter(sample %in% c(\"Without a Caliper\", \"With a 0.5 SD Caliper\")) %>%\r\n  mutate(\r\n    matching_procedure = ifelse(\r\n      sample == \"Without a Caliper\",\r\n      \"Propensity Score Matching without a Caliper\",\r\n      \"Propensity Score Matching with a 0.5 SD Caliper\"\r\n    )) %>%\r\n  select(-sample)\r\n\r\n# bind clean data\r\ndata_cov_balance <- bind_rows(data_initial, data_ps) %>%\r\n  bind_rows(., data_cm_cpm)\r\n\r\n\r\n\r\nWe compute summary statistics on balance for continuous variables:\r\n\r\nMatching Procedure\r\nAverage of Standardized Mean Differences\r\nStd. Deviation\r\nInitial Data\r\n0.51\r\n0.30\r\nPropensity Score Matching without a Caliper\r\n0.06\r\n0.04\r\nPropensity Score Matching with a 0.5 SD Caliper\r\n0.06\r\n0.02\r\nCoarsened Exact Matching\r\n0.04\r\n0.04\r\nConstrained Pair Matching\r\n0.07\r\n0.05\r\n\r\nWe compute summary statistics on balance for binary variables:\r\n\r\nMatching Procedure\r\nAverage of Proportion Differences\r\nStd. Deviation\r\nInitial Data\r\n0.06\r\n0.08\r\nPropensity Score Matching without a Caliper\r\n0.02\r\n0.02\r\nPropensity Score Matching with a 0.5 SD Caliper\r\n0.01\r\n0.01\r\nCoarsened Exact Matching\r\n0.02\r\n0.03\r\nConstrained Pair Matching\r\n0.03\r\n0.03\r\n\r\nAnalysis Results\r\nWe load the results on the estimates of treatment effect sizes for the matching procedures and the regression outcome approach:\r\n\r\n\r\n# load and bind data\r\nfiles <- dir(\r\n  path = here::here(\"inputs\",\r\n                    \"3.outputs\",\r\n                    \"1.data\",\r\n                    \"analysis_results\"),\r\n  pattern = \"*.RDS\",\r\n  full.names = TRUE\r\n)\r\n\r\ndata_analysis_results <- files %>%\r\n  map(~ readRDS(.)) %>% \r\n  reduce(rbind)  \r\n\r\n\r\n\r\nWe display below the summary of results in a table:\r\n\r\nProcedure\r\nSample Size\r\nEstimate\r\n95% CI\r\nWidth of CI\r\nOutcome Regression Model without Covariates Adjustment\r\n1376\r\n387\r\n(296; 479)\r\n183\r\nOutcome Regression Model with Covariates Adjustment\r\n1376\r\n243\r\n(145; 340)\r\n195\r\nPropensity Score without a Caliper\r\n244\r\n261\r\n(114; 409)\r\n295\r\nPropensity Score without a Caliper and with Covariates Adjustment\r\n244\r\n241\r\n(101; 380)\r\n279\r\nPropensity Score with a 0.5 SD Caliper\r\n236\r\n265\r\n(112; 418)\r\n306\r\nPropensity Score with a 0.5 SD Caliper and with Covariates Adjustment\r\n236\r\n251\r\n(103; 399)\r\n296\r\nCoarsened Matching without Covariates Adjustment\r\n158\r\n207\r\n(60; 354)\r\n294\r\nCoarsened Matching with Covariates Adjustment\r\n158\r\n297\r\n(154; 440)\r\n286\r\nConstrained Pair Matching without Covariates Adjustment\r\n88\r\n370\r\n(190; 550)\r\n360\r\nConstrained Pair Matching with Covariates Adjustment\r\n88\r\n335\r\n(159; 511)\r\n352\r\n\r\nWe display below the summary of results in a graph:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-03-27T14:41:49+02:00"
    }
  ],
  "collections": []
}
