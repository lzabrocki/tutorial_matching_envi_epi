---
title: "Constrained Pair Matching"
description: |
  Detailled Script.
author:
  - name: Tarik Benmarhnia
    url: https://profiles.ucsd.edu/tarik.benmarhnia
    affiliation: UCSD & Scripps Institute
    affiliation_url: https://benmarhniaresearch.ucsd.edu/
  - name: Marie-Abèle Bind 
    url: https://scholar.harvard.edu/marie-abele
    affiliation: Biostatistics Center, Massachusetts General Hospital
    affiliation_url: https://biostatistics.massgeneral.org/faculty/marie-abele-bind-phd/
  - name: Léo Zabrocki 
    url: https://lzabrocki.github.io/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/fr/zabrocki-leo/
date: "`r Sys.Date()`"
output: 
    distill::distill_article:
      toc: true
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# code chunk option
knitr::opts_chunk$set(
  fig.path = "images/",
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  dev = "CairoPNG",
  dpi = 400
)
```

<style>
body {
text-align: justify}
</style>


In this document, we provide all steps and R codes required to estimate the effect of heat waves of the number of years of life lost (YLL) using a recently developed constrained pair matching algorithm. Compared to propensity score matching, we can choose the maximum distance allowed between treated and control units for each covariate. The coding procedure is a bit more involved as it has not been formatted in an R package yet. **Should you have any questions, need help to reproduce the analysis or find coding errors, please do not hesitate to contact us at leo.zabrocki@psemail.eu**

# Required Packages and Data Loading

To reproduce exactly the `constrained_pair_matching.html` document, we first need to have installed:

* the [R](https://www.r-project.org/) programming language 
* [RStudio](https://rstudio.com/), an integrated development environment for R, which will allow you to knit the `constrained_pair_matching.Rmd` file and interact with the R code chunks
* the [R Markdown](https://rmarkdown.rstudio.com/) package
* and the [Distill](https://rstudio.github.io/distill/) package which provides the template for this document. 

Once everything is set up, we load the following packages:

```{r}
# load required packages
library(knitr) # for creating the R Markdown document
library(here) # for files paths organization
library(tidyverse) # for data manipulation and visualization
library(broom) # for cleaning regression outputs
library(Rcpp) # for running the matching algorithm
library(optmatch) # for matching pairs
library(igraph) # for maximum bipartite matching
library(lmtest) # for modifying regression standard errors
library(sandwich) # for robust and cluster robust standard errors
library(Cairo) # for printing custom police of graphs
library(DT) # for displaying the data as tables
```

We also have to load the `script_time_series_matching_function.R` located in the **functions** folder and which provides the functions used for matching time series:

```{r}
# load matching functions
source(here::here(
  "docs",
  "2.functions",
  "script_time_series_matching_function.R"
))
```

We load our custom `ggplot2` theme for graphs:

```{r}
# load ggplot custom theme
source(here::here("docs",
                  "2.functions",
                  "script_theme_tufte.R"))
# define nice colors
my_blue <- "#0081a7"
my_orange <- "#fb8500"
```

We finally load the data:

```{r}
# load the data
matching_data <-
  readRDS(here::here("docs", "1.data", "environmental_data.rds")) %>%
  fastDummies::dummy_cols(., select_columns = c("month", "year")) %>%
  mutate(month = as.factor(month) %>% as.numeric(.))
```

# Matching Procedure

### Defining the Treatment of Interest

We defined our experiment such that:

* treated units are days where an heat wave occurred in *t*.
* control units are day no heat wave occurred in *t*.

Below are the required steps to define treatment variable (`is_treated`) and select the corresponding treated and control units:

```{r}
# define treatment variable
matching_data <- matching_data %>%
  mutate(is_treated = ifelse(heat_wave == 1, TRUE, FALSE))

# subset treated and control units
treated_units = subset(matching_data, is_treated)
control_units = subset(matching_data,!is_treated)
N_treated = nrow(treated_units)
N_control = nrow(control_units)
```

There are `r N_treated` treated units and  `r N_control` control units. We display the distribution of of treated and control units through time:

```{r, fig.width=8, fig.height=5, code_folding="Please show me the code!"}
# make stripes graph
matching_data %>%
  mutate(is_treated = ifelse(is_treated == "TRUE", "Treated", "Control")) %>%
  ggplot(., aes(x = date, y = 1, fill = is_treated)) +
  geom_tile() +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~ year, scales = "free") +
  scale_fill_manual(name = "Daily Observations:", values = c(my_blue, my_orange)) +
  xlab("Date") + ylab("") +
  theme_tufte() +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank())
```

We save the `matching_data`:

```{r}
saveRDS(
  matching_data,
  here::here(
    "docs", 
    "3.outputs",
    "1.data",
    "constrained_pair_matching",
    "matching_data.rds"
  )
)
```

### Defining Thresholds for Matching Covariates

For each relevant covariate, we must define a maximum distance for which a treated unit can be matched to a control unit. We chose the distance to maximize covariates balance but also reach a number of matched treated units large enough to draw our inference upon:

* A treated unit can be matched to a control unit for a maximum distance of 4 years.
* A treated unit must be matched to a control unit according to weekend/working days.
* A treated unit must be matched to a control unit belonging to the same month.
* A treated unit must be matched to a control unit for the same three lags of the heat wave dummy.
* A treated unit can be matched to a control unit for a maximum distance of 10 percentage points in relative humidity.
* A treated unit can be matched to a control unit for a maximum distance of 12 $\mu g/m^3$ in the three lags of ozone.
* A treated unit can be matched to a control unit for a maximum distance of 0 $\mu g/m^3$ in the three lags of nitrogen dioxide.

Below is the code to define the relevant thresholds:

```{r}
# we create the scaling list as it is needed for running the algorithm
# but we do not use it

scaling =  rep(list(1),ncol(matching_data))
names(scaling) = colnames(matching_data)

# instead, we manually defined the threshold for each covariate
thresholds = rep(list(Inf),ncol(matching_data))
names(thresholds) = colnames(matching_data)

# threshold for year
thresholds$year = 2

# threshold for weekend
thresholds$weekend = 0

# thresholds for month
thresholds$month = 0

# threshold for heat_wave lags
thresholds$heat_wave_lag_1 = 0
thresholds$heat_wave_lag_2 = 0
thresholds$heat_wave_lag_3 = 0

# threshold for humidity_relative
thresholds$humidity_relative = 10

# threshold for o3
thresholds$o3_lag_1 = 12
thresholds$o3_lag_2 = 12
thresholds$o3_lag_3 = 12

# thresholds for no2
thresholds$no2_lag_1 = 9
thresholds$no2_lag_2 = 9
thresholds$no2_lag_3 = 9
```

### Running the Matching Procedure

We compute discrepancy matrix and run the matching algorithm:

```{r}
# first we compute the discrepancy matrix
discrepancies = discrepancyMatrix(treated_units, control_units, thresholds, scaling)

# convert matching data to data.frame
matching_data <- as.data.frame(matching_data)

rownames(discrepancies) = format(matching_data$date[which(matching_data$is_treated)],"%Y-%m-%d")
colnames(discrepancies) = format(matching_data$date[which(!matching_data$is_treated)],"%Y-%m-%d")
rownames(matching_data) = matching_data$date

# run the fullmatch algorithm
matched_groups = fullmatch(discrepancies, data = matching_data, remove.unmatchables = TRUE, max.controls = 1)

# get list of matched  treated-control groups
groups_labels = unique(matched_groups[!is.na(matched_groups)])
groups_list = list()
for (i in 1:length(groups_labels)){
  IDs = names(matched_groups)[(matched_groups==groups_labels[i])]
  groups_list[[i]] = as.Date(IDs[!is.na(IDs)])
}
```

For some cases, several controls units were matched to a treatment unit. We use the `igraph` package to force pair matching via bipartite maximal weighted matching. Below is the required code:

```{r}
# we build a bipartite graph with one layer of treated nodes, and another layer of control nodes.
# the nodes are labeled by integers from 1 to (N_treated + N_control)
# by convention, the first N_treated nodes correspond to the treated units, and the remaining N_control
# nodes correspond to the control units.

# build pseudo-adjacency matrix: edge if and only if match is admissible
# NB: this matrix is rectangular so it is not per say the adjacendy matrix of the graph
# (for this bipartite graph, the adjacency matrix had four blocks: the upper-left block of size
# N_treated by N_treated filled with 0's, bottom-right block of size N_control by N_control filled with 0's,
# top-right block of size N_treated by N_control corresponding to adj defined below, and bottom-left block
# of size N_control by N_treated corresponding to the transpose of adj)
adj = (discrepancies<Inf)

# extract endpoints of edges
edges_mat = which(adj,arr.ind = TRUE)

# build weights, listed in the same order as the edges (we use a decreasing function x --> 1/(1+x) to
# have weights inversely proportional to the discrepancies, since maximum.bipartite.matching
# maximizes the total weight and we want to minimize the discrepancy)
weights = 1/(1+sapply(1:nrow(edges_mat),function(i)discrepancies[edges_mat[i,1],edges_mat[i,2]]))

# format list of edges (encoded as a vector resulting from concatenating the end points of each edge)
# i.e c(edge1_endpoint1, edge1_endpoint2, edge2_endpoint1, edge2_endpoint1, edge3_endpoint1, etc...)
edges_mat[,"col"] = edges_mat[,"col"] + N_treated
edges_vector = c(t(edges_mat))

# NB: by convention, the first N_treated nodes correspond to the treated units, and the remaining N_control
# nodes correspond to the control units (hence the "+ N_treated" to shift the labels of the control nodes)

# build the graph from the list of edges
BG = make_bipartite_graph(c(rep(TRUE,N_treated),rep(FALSE,N_control)), edges = edges_vector)

# find the maximal weighted matching
MBM = maximum.bipartite.matching(BG, weights = weights)

# list the dates of the matched pairs
pairs_list = list()
N_matched = 0
for (i in 1:N_treated){
  if (!is.na(MBM$matching[i])){
    N_matched = N_matched + 1
    pairs_list[[N_matched]] = c(treated_units$date[i],control_units$date[MBM$matching[i]-N_treated])
  }
}

# transform the list of matched pairs to a dataframe
matched_pairs <- enframe(pairs_list) %>%
  unnest(cols = "value") %>%
  rename(pair_number = name,
         date = value)
```

The hypothetical experiment we set up had `r N_treated` treated units and `r N_control` control units. The matching procedure results in `r N_matched` matched treated units.

We finally merge the `matched_pairs` with the `matching_matching_data` to retrieve covariates values for the matched pairs and save the data:

```{r}
# select the matched data for the analysis
final_data <-
  left_join(matched_pairs, matching_data, by = "date") %>%
  mutate(pair_number = as.factor(pair_number))

# save the matched data
saveRDS(
  final_data,
  here::here(
    "docs",
    "3.outputs",
    "1.data",
    "constrained_pair_matching",
    "matched_data.Rds"
  )
)
```

# Checking Covariates Balance Improvement

We first bind the matching and matched data together:

```{r}
# bind the two datasets
matching_data <- matching_data %>%
  mutate(dataset = "Initial Data")

final_data <- final_data %>%
    mutate(dataset = "Matched Data")

data <- bind_rows(matching_data, final_data)
```

We change labels of the `is_treated` variable :

```{r}
data <- data %>%
  mutate(is_treated = ifelse(is_treated == "TRUE", "True", "False"))
```

We then assess covariates balance using a love plot:

```{r, fig.width=10, fig.height=8, code_folding="Please show me the code!"}
# compute absolute standardized mean differences for continuous covariates
data_cov_continuous <- data %>%
  select(dataset, is_treated, humidity_relative, o3_lag_1:o3_lag_3, no2_lag_1:no2_lag_3) %>%
  pivot_longer(
    cols = -c(is_treated, dataset),
    names_to = "variable",
    values_to = "values"
  ) %>%
  select(dataset, is_treated, variable, values)

data_abs_difference_continuous <- data_cov_continuous %>%
  group_by(dataset, variable, is_treated) %>%
  summarise(mean_values = mean(values, na.rm = TRUE)) %>%
  summarise(abs_difference = abs(mean_values[2] - mean_values[1]))

data_sd_continuous <- data_cov_continuous %>%
  filter(dataset == "Initial Data" & is_treated == "True") %>%
  group_by(variable) %>%
  summarise(sd_treatment = sd(values, na.rm = TRUE)) %>%
  ungroup() %>%
  select(variable, sd_treatment)

data_love_continuous <-
  left_join(data_abs_difference_continuous, data_sd_continuous, by = c("variable")) %>%
  mutate(standardized_difference = abs_difference / sd_treatment) %>%
  select(-c(abs_difference, sd_treatment)) %>%
  mutate(type = "continuous")


# compute absolute raw mean differences for binary covariates
data_cov_binary <- data %>%
  select(dataset, is_treated, heat_wave_lag_1:heat_wave_lag_3, weekend, month_june:year_2007) %>%
  pivot_longer(
    cols = -c(is_treated, dataset),
    names_to = "variable",
    values_to = "values"
  ) %>%
  select(dataset, is_treated, variable, values)

data_love_binary <- data_cov_binary %>%
  group_by(dataset, variable, is_treated) %>%
  summarise(mean_values = mean(values, na.rm = TRUE)) %>%
  summarise(standardized_difference = abs(mean_values[2] - mean_values[1])) %>%
  mutate(type = "binary")


# combine the two datasets
data_love <- bind_rows(data_love_continuous, data_love_binary)

# add variable labels
data_love <- data_love %>%
  mutate(
    variable = case_when(
      variable == "heat_wave_lag_1" ~ "Heat Wave t-1",
      variable == "heat_wave_lag_2" ~ "Heat Wave t-2",
      variable == "heat_wave_lag_3" ~ "Heat Wave t-3",
      variable == "humidity_relative" ~ "Relative Humidity*",
      variable == "weekend" ~ "Weekend",
      variable == "month_august" ~ "August",
      variable == "month_july" ~ "July",
      variable == "month_june" ~ "June",
      variable == "no2_lag_1" ~ "NO2 t-1*",
      variable == "no2_lag_2" ~ "NO2 t-2*",
      variable == "no2_lag_3" ~ "NO2 t-3*",
      variable == "o3_lag_1" ~ "O3 t-1*",
      variable == "o3_lag_2" ~ "O3 t-2*",
      variable == "o3_lag_3" ~ "O3 t-3*",
      variable == "year_1990" ~ "1990",
      variable == "year_1991" ~ "1991",
      variable == "year_1992" ~ "1992",
      variable == "year_1993" ~ "1993",
      variable == "year_1994" ~ "1994",
      variable == "year_1995" ~ "1995",
      variable == "year_1996" ~ "1996",
      variable == "year_1997" ~ "1997",
      variable == "year_1998" ~ "1998",
      variable == "year_1999" ~ "1999",
      variable == "year_2000" ~ "2000",
      variable == "year_2001" ~ "2001",
      variable == "year_2002" ~ "2002",
      variable == "year_2003" ~ "2003",
      variable == "year_2004" ~ "2004",
      variable == "year_2005" ~ "2005",
      variable == "year_2006" ~ "2006",
      variable == "year_2007" ~ "2007"
    )
  )

# arrange the dataset
data_love <- data_love %>%
  arrange(dataset, standardized_difference) %>% 
  mutate(variable=factor(variable, levels=variable))

# make the graph
graph_cpm_love_plot_cobalt <- ggplot(data_love, aes(y = variable, x = standardized_difference, colour = fct_rev(dataset), shape = fct_rev(dataset))) +
  geom_vline(xintercept = 0, size = 0.3) +
  geom_vline(xintercept = 0.1, color = "black", linetype = "dashed") +
  geom_point(size = 4, alpha = 0.8) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 12)) +
  scale_colour_manual(name = "Dataset:", values = c(my_blue, my_orange)) +
  scale_shape_manual(name = "Dataset:", values = c(17, 16)) +
  xlab("Standardized Mean Differences") +
  ylab("") + 
  theme_tufte() + 
  theme(plot.margin = margin(t = 0.25, r = 5, b = 0.25, l = 0, unit = "cm"))

# display the graph
graph_cpm_love_plot_cobalt

# save the graph
ggsave(
  graph_cpm_love_plot_cobalt,
  filename = here::here(
    "docs",
    "3.outputs",
    "2.graphs",
    "graph_cpm_love_plot_cobalt.pdf"
  ),
  width = 30,
  height = 18,
  units = "cm",
  device = cairo_pdf
)
```

We display below the evolution of the average of standardized mean differences for continuous covariates:

```{r, code_folding="Please show me the code!"}
data_love %>%
  filter(type == "continuous") %>%
  group_by(dataset) %>%
  summarise("Average of Standardized Mean Differences" = round(mean(standardized_difference), 2),
            "Std. Deviation" = round(sd(standardized_difference), 2)) %>%
  kable(align = c("l", "c"))
```

We also display below the evolution of the difference in proportions for binary covariates:

```{r, code_folding="Please show me the code!"}
data_love %>%
  filter(type == "binary") %>%
  group_by(dataset) %>%
  summarise("Average of Proportion Differences" = round(mean(standardized_difference), 2),
            "Std. Deviation" = round(sd(standardized_difference), 2)) %>%
  kable(align = c("l", "c"))
```

Overall, the balance has improved for continuous covariates after matching. It is not really the case for binary variables.

We finally save the data on covariates balance in the `3.outputs/1.data/covariates_balance` folder.

```{r, code_folding="Please show me the code!"}
data_love %>%
  rename(sample = dataset, var = variable, stat = standardized_difference) %>%
  mutate(matching_procedure = "Constrained Pair Matching") %>%
  saveRDS(
    .,
    here::here(
          "docs",
      "3.outputs",
      "1.data",
      "covariates_balance",
      "data_cov_balance_cpm.RDS"
    )
  )
```

# Analysing Results

We must be careful when we analyze the results as only `r N_matched` treated units were matched: the estimand is no longer the ATT. We compute the estimate using a simple linear regression model:

```{R, echo=TRUE}
# we fit the regression model
model_cpm_wo_cov <-
  lm(
    yll ~ is_treated,
    data = final_data
  )

# retrieve the estimate and 95% ci
results_cpm_wo_cov <- tidy(coeftest(
  model_cpm_wo_cov,
  vcov. = vcovCL,
  cluster = ~ pair_number
), conf.int = TRUE) %>%
  filter(term == "is_treatedTRUE") %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate_at(vars(estimate:conf.high), ~ round(., 0))

# display results
results_cpm_wo_cov %>%
  rename(
    "Term" = term,
    "Estimate" = estimate,
    "95% CI Lower Bound" = conf.low,
    "95% CI Upper Bound" = conf.high
  ) %>%
  kable(., align = c("l", "c", "c", "c"))
```

We find that the average effect on the treated is equal to +`r results_cpm_wo_cov$estimate` years of life lost. The 95% confidence interval is however wide and is consistent with effects ranging from +`r results_cpm_wo_cov$conf.low` up to +`r results_cpm_wo_cov$conf.high`. If we want to increase the precision of our estimate and remove any remaining imbalance in covariates, we can also run a multivariate regression. We adjust below for the same variables used in the propensity score matching procedure and the day of the week:

```{r}
# we fit the regression model
model_cpm_w_cov <-
  lm(
    yll ~ heat_wave + o3_lag_1 + o3_lag_2,
    data = final_data)

# retrieve the estimate and 95% ci
results_cpm_w_cov <- tidy(coeftest(
  model_cpm_w_cov,
  vcov. = vcovCL,
  cluster = ~ pair_number
), conf.int = TRUE) %>%
  filter(term == "heat_wave") %>%
  select(term, estimate, conf.low, conf.high) %>%
  mutate_at(vars(estimate:conf.high), ~ round(., 0))

# display results
results_cpm_w_cov %>%
  rename(
    "Term" = term,
    "Estimate" = estimate,
    "95% CI Lower Bound" = conf.low,
    "95% CI Upper Bound" = conf.high
  ) %>%
  kable(., align = c("l", "c", "c", "c"))
```

We find that the average effect on the treated is equal to +`r results_cpm_w_cov$estimate` years of life lost. The 95% confidence interval is consistent with effects ranging from +`r results_cpm_w_cov$conf.low` up to +`r results_cpm_w_cov$conf.high`. The width of confidence interval is now equal to `r results_cpm_w_cov$conf.high - results_cpm_w_cov$conf.low`, which is smaller than the previous interval of `r results_cpm_wo_cov$conf.high - results_cpm_wo_cov$conf.low`.

We finally save the data on constrained pair matching results in the `3.outputs/1.data/analysis_results` folder.

```{r, code_folding="Please show me the code!"}
bind_rows(results_cpm_wo_cov, results_cpm_w_cov) %>%
  mutate(
    procedure = c("Constrained Pair Matching without Covariates Adjustment", "Constrained Pair Matching with Covariates Adjustment"),
    sample_size = rep(N_matched*2, 2)
  ) %>%
  saveRDS(
    .,
    here::here(
          "docs",
      "3.outputs",
      "1.data",
      "analysis_results",
      "data_analysis_cpm.RDS"
    )
  )
```
